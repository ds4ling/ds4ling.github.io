<!DOCTYPE html>
<html>
  <head>
    <title>Data Science for Linguists</title>
    <meta charset="utf-8">
    <meta name="author" content="Joseph V. Casillas, PhD" />
    <link href="assets/remark-css/hygge.css" rel="stylesheet" />
    <link href="assets/remark-css/rutgers.css" rel="stylesheet" />
    <link href="assets/remark-css/rutgers-fonts.css" rel="stylesheet" />
    <script src="https://use.fontawesome.com/5235085b15.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Data Science for Linguists
## The Generalized Linear Model
### Joseph V. Casillas, PhD
### Rutgers University</br>Spring 2018</br>Last update: 2018-04-17

---













class: title-slide-section-grey, middle

# Review

---
background-image: url(./assets/img/01_distributions.png)
background-size: 650px
background-position: 5% 10%

--

background-image: url(./assets/img/02_clt.png)
background-size: 650px
background-position: 5% 10%

--

background-image: url(./assets/img/03a_nhst.png)
background-size: 700px
background-position: 50% 50%

--

background-image: url(./assets/img/03b_hypothesis_testing.png)
background-size: 700px
background-position: 50% 50%

--

background-image: url(./assets/img/03c_ttest.png)
background-size: 800px
background-position: 50% 50%

--

background-image: url(./assets/img/05_correlation.png)
background-size: 500px
background-position: 90% 10%

--

background-image: url(./assets/img/06a_bvar_reg.png)
background-size: 550px
background-position: 0% 90%

--

background-image: url(./assets/img/06b_bvar_reg.png)
background-size: 550px
background-position: 0% 90%

--

background-image: url(./assets/img/07_general_lm01.png)
background-size: 550px
background-position: 100% 90%

---
background-image: url(./assets/img/01_distributions.png), url(./assets/img/03a_nhst.png), url(./assets/img/05_correlation.png), url(./assets/img/06b_bvar_reg.png), url(./assets/img/07_general_lm01.png)
background-size: 400px, 400px, 400px, 400px, 400px
background-position: 5% 10%, 50% 50%, 95% 10%, 0% 95%, 100% 95%

---

# Review

.pull-left[

### What we know, where we've been

- Distributions
  - Normal distribution
  - CLT
- Hypothesis testing
  - z-tests
  - t-tests
- Bivariate correlation
- The linear model
  - Bivariate regression
  - Multiple regression and correlation
- The general linear model
  - Continuous predictors
  - Categorical predictors

]

--

.pull-right[

### What they have in common

- Criterion
  - Continuous
  - Linear relationship with predictors
  - `\(-\infty\)` : `\(\infty\)`
  - Errors are normally distributed

]

---

# (P)review

### Where we're headed

- Sometimes we measure phenomena that are not continuous variables ranging from 
  `\(-\infty\)` : `\(\infty\)`

&lt;p&gt;&lt;/p&gt;

- For example, we often analyze binary outcomes
  - Decisions
  - The presence/absence of a linguistic feature
  - Categorical perception 

&lt;p&gt;&lt;/p&gt;

- Sometimes we count things
  - Number of languages in a given area
  - Number of code switches during a linguistic interview

&lt;p&gt;&lt;/p&gt;

- We can extend our model in order to account for these different types of 
*dependent* variables

---
class: title-slide-section-grey, middle

# The generalized linear model

---

# The generalized linear model

### History

- Formulated by Nelder and Wedderburn (1972)
- The purpose was to unify (again) the different models being used at the time

### How

Recall that linear models contain a *systematic component* that specifies 
predictors (*X*&lt;sub&gt;1&lt;/sub&gt;, *X*&lt;sub&gt;2&lt;/sub&gt; ... *X*&lt;sub&gt;*k*&lt;/sub&gt;), which, 
in turn, create our linear predictor (β&lt;sub&gt;0&lt;/sub&gt; + β&lt;sub&gt;1&lt;/sub&gt;*x*&lt;sub&gt;1&lt;/sub&gt; ... β&lt;sub&gt;k&lt;/sub&gt;*x*&lt;sub&gt;k&lt;/sub&gt;). At a minimum, a GLM contains the following: 

.pull-left[

1) .blue[A data *distribution*]. This refers to the probability distribution of 
the response variable.

2) .blue[A *linking function*] that transforms the criterion used to model the 
data. It *links* the data distribution of the response variable to the 
systematic component of the model.

]

--

.pull-right[

3) .blue[An *estimator*], or method for obtaining parameter estimates

You (the researcher) are responsible for selecting (1) and (2). (3) is always 
the same.

]

---
background-image: url(./assets/img/glm_distributions.png)
background-size: 600px
background-position: 95% 50%

# The generalized linear model

### Distributions

#### Meet the (exponential) family

The exponential family is a series of probability  
distributions, each with its own properties. 

There are +/- 12, but we will focus on 3:

- Gaussian 

- Binomial 

- Poisson 

---

# The generalized linear model

### Linking functions

The linking function transforms the response variable in the manner most 
appropriate given the data distribution you have selected. 

- Identity: for gaussian response variable (normal distribution)

- Logit: for binary response variable (binomial distribution)

- Log: for count response variable (poisson distribution)

&lt;/br&gt;

--

(more detail later)

---

# The generalized linear model

### The estimator: Maximum Likelihood Estimation (MLE)

&lt;ru-blockquote&gt;
Maximum likelihood estimation is the method that determines the values of the
parameters of the model. The parameter estimates are obtained in a way that 
maximizes the likelihood that the process described by the model produced 
the data that were actually observed.&lt;sup&gt;1&lt;/sup&gt;
&lt;/ru-blockquote&gt;

&lt;img src="index_files/figure-html/mle_ex-1.png" width="720" style="display: block; margin: auto;" /&gt;

.footnote[&lt;sup&gt;1&lt;/sup&gt; [Brooks-Bartlett](https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1) (2018)]

---
background-image: url(./assets/img/variance_explained2-1.png)
background-size: contain
background-position: 100%
class: middle

.pull-left[

## Recall that classical linear regression uses .RUred[*ordinary least squares estimation*] to determine the line that minimizes the residual sum of squares.

]

--

.footnote[You don't need to know the details behind MLE,  
but it never hurts to understand what is going on  
under the hood. [Brooks-Bartlett](https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1) (2018)  
is a good place to start if you are interested.]

---

# The generalized linear model

.pull-left[

### Something old

- It turns out we've spent the previous 11 weeks working with the gaussian 
distribution
- Gaussian is another way of saying normal
- This means we can think of standard linear regression in the general linear model as a special case of the generalized linear model
  - The errors are normally (gaussian) distributed
  - The criterion is associated with the linear predictors via an identity 
  linking function 
  - Instead of least squares estimation we use maximum likelihood estimation

]

--

.pull-right[

### Something new

- We can use different combinations of distributions and linking functions to 
model different outcome variables
- There are many options. Learning new modeling tools will open your mind to 
new experimental possibilities (if you only have a hammer, everything is a nail)
- We will focus on two types of regression that can be fit in the Generalized 
Linear Model framework: 
  1. Logistic regression (outcome variable is binary)
  2. Poisson regression (outcome variable represents counts)

]

---

# The generalized linear model

### Assumptions

- Data are independently distributed (independence of scores)

- Dependent variable follows a distribution from the exponential family

- Linear relationship between transformed response variable and predictors 
(linear relationship is result of linking function)

- Errors are independent (NOTE: they do not have to be normally distributed)

---

# The generalized linear model

### Doing it in R

#### The `glm()` function

- Thus far we have used the `lm()` function to fit models

- The `glm()` function works in the same way

--


```r
my_glm &lt;- glm(criterion ~ pred1 + pred2 + pred1:pred2, # model formula
              data = my_df,                            # select dataframe
*             family = gaussian(link = "identity"))    # dist. and link func.
```

--

- Steps
  1. Specify the .blue[model formula]: `criterion ~ pred1 + pred2 + pred1:pred2`
  2. Select the .red[dataframe]: `data = my_df`
  3. Select a .green[distribution family] and .green[linking function]: 
  `family = gaussian(link = "identity")`

---

# The generalized linear model

.pull-left[


```r
# Standard lm()
lm(mpg ~ drat, data = mtcars)
```

&lt;table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -7.525 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.477 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.374 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.18 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; drat &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.678 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.507 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.096 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]

.pull-right[


```r
glm(mpg ~ drat, data = mtcars, 
    family = gaussian(link = "identity"))
```

&lt;table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -7.525 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.477 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.374 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.18 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; drat &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.678 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.507 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.096 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]

--

&lt;/br&gt;

&lt;div align="center"&gt;
&lt;img width="450" src="./assets/img/russian_dolls.png"&gt;
&lt;/div&gt;

.center[**The standard linear model is a special case of the GLM**]

---
background-color: black
class: middle

&lt;div style="float:right"&gt;

```
## Warning in is.na(x): is.na() applied to non-(list or vector) of type
## 'expression'
```

&lt;img src="index_files/figure-html/unnamed-chunk-5-1.png" width="576" style="display: block; margin: auto 0 auto auto;" /&gt;
&lt;/div&gt;

# Logistic regression

---

# Logistic regression

### Model setup

- The criterion is binary (0/1)

- The distribution for binary data is the **binomial** distribution

- The linking function is the **logit**

### How it works

- The logit linking function transforms the criterion so that it can be modeled by the linear predictor

`$$logit(p) = log(\frac{p}{1 - p})$$`

- In other words, the linking function transforms the 0/1 outcomes to 
`\(-\infty\)` : `\(\infty\)`

`$$log(\frac{p_{i}}{1 - p_{i}}) = \alpha + \beta_{1}X_{1} + \beta_{2}X_{2} + ... + \beta_{k}X_{k}$$`

---

&lt;img src="index_files/figure-html/lin_log_comp-1.png" width="1008" /&gt;

---

# Logistic regression

### What you need to know

- Logistic regression is the most appropriate way to model binary response 
variables (0/1)

- The model calculates the probability that y = 1, i.e., the probability of a 
"success", or presence of something

- Model output from logistic regression is similar to `lm()`

- Model interpretation "works" the same way, i.e., a 1-unit change in 
`predictor` is associated with a change of X in the criterion

- But... the parameter estimates represent changes in the log-odds of y = 1

- This is much less intuitive, much more difficult to understand without some 
math

---

# Logistic regression

### Example

- You are interested in understanding the perception of stop voicing in 
English bilabials 

- You conducted an experiment in which participants heard a range of 
bilabial stops that differed in voice-onset time

- The stimuli ranged from -60 ms to 60 ms in 10 ms increments 

- Participants were presented stimuli drawn at random from the continuum and 
identified the sounds as /b/'s or /p/'s

- A /p/ response is coded as a 1

--

.center[![:scale 70%](./assets/img/vot.png)]

---

# Logistic regression

### Example


```r
mod_log &lt;- glm(resp ~ vot, data = logistic_df, family = binomial(link = "logit"))
```


```
## 
## Call:
## glm(formula = resp ~ vot, family = "binomial", data = logistic_df)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3085  -0.6583  -0.2198   0.6503   2.9320  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.84614    0.06089   -13.9   &lt;2e-16 ***
## vot          0.05731    0.00213    26.9   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 3482.8  on 2599  degrees of freedom
## Residual deviance: 2063.6  on 2598  degrees of freedom
## AIC: 2067.6
## 
## Number of Fisher Scoring iterations: 5
```

---

# Logistic regression

### Example

- We can convert the log-odds to probabilities by calculating the inverse 
logit&lt;sup&gt;1&lt;/sup&gt;


```r
inv_logit(mod_log) %&gt;% kable(., format = 'html')
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; variables &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; betas &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; prob &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.8461443 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.3002423 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; vot &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0573077 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5143230 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

--

- This is still difficult to interpret... a plot might help. 

&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;

&lt;sup&gt;1&lt;/sup&gt; Inverse logit = `\(\frac{1}{1 + exp(-x)}\)`

---
exclude: true

&lt;img src="index_files/figure-html/vot_plot-1.png" width="576" /&gt;

---
background-image: url(./index_files/figure-html/vot_plot-1.png)
background-size: contain

---
background-image: url(./index_files/figure-html/vot_plot-1.png)
background-size: contain
background-position: 100%

.pull-left[

```r
inv_logit(mod_log) %&gt;% kable(., format = 'html')
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; variables &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; betas &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; prob &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.8461443 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.3002423 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; vot &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0573077 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5143230 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;/br&gt;

- Now the intercept is interpretable  
(note it is already centered)

- What does the parameter estimate  
for VOT mean?

- Can calculate how the probability  
differs from one specific point to  
another?

]

---
background-image: url(./index_files/figure-html/vot_plot-1.png)
background-size: 600px
background-position: 100%

.pull-left[

#### We can use the model coefficients

&lt;table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.846 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.061 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -13.897 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; vot &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.057 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.002 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 26.899 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

- Calculate the inverse logit of the  
linear equation:  

`$$\alpha + \beta_{VOT} * 10ms$$`


```r
inv_logit_manual(-0.846 + 0.057 * 10)
```

```
## [1] 0.4314347
```

- What about the change in probability of selecting /p/ when shifting from 10 ms 
to 20 ms?


```r
inv_logit_manual(-0.846 + 0.057 * 20) - 
inv_logit_manual(-0.846 + 0.057 * 10)
```

```
## [1] 0.1415404
```

The shift from 10ms to 20 ms VOT corresponds  
with a positive difference of 14% in 
the probability of selecting /p/

]

---

# Logistic regression

### Summary

- Logistic regression is a powerful tool for modeling binary data

- The `glm()` function works similarly to the `lm()` function 

- We test for main effects and interactions the same way too, i.e., using 
nested model comparisons with the `anova()` function

- The exponential family and corresponding linking function are  
`family = binomial(link = "logit")`

- Interpretation of logistic regression works the same way as classic linear 
regression 

- Parameter estimates are evaluated in log odds (and require some work in order 
to accurately interpret them)

---

# Practice

- [Logistic regression practice](./assets/logistic_regression_walkthrough/glm_logistic.zip)

---






















background-color: black
class: middle

&lt;div style="float:right"&gt;

```
## Warning in is.na(x): is.na() applied to non-(list or vector) of type
## 'expression'
```

&lt;img src="index_files/figure-html/unnamed-chunk-7-1.png" width="576" style="display: block; margin: auto 0 auto auto;" /&gt;
&lt;/div&gt;

# Poisson regression

---

# Poisson regression

### Model setup

- The criterion is a non-negative number (0, 1, 2...)

- The distribution for count data is typically the **poisson** distribution

- The linking function is **log** 

--


### How it works

- The log linking function transforms the criterion so that it can be modeled by 
the linear predictor

`$$g(u) = exp(u)$$`

- Conversely, this can be understood as exponentiating the linear predictor to 
make the values non-negative

`$$y_{i} = exp(\alpha + \beta_{1}X_{1} + \beta_{2}X_{2} + ... + \beta_{k}X_{k})$$`

---

&lt;img src="index_files/figure-html/lin_poisson_comp-1.png" width="1008" /&gt;

---

# Poisson regression

### What you need to know

- Poisson regression is the most appropriate way to model count data 
(0, 1, 2...)

- Model output from poisson regression is similar to `lm()`

- Model interpretation "works" the same way, i.e., a 1-unit change in 
`predictor` is associated with a change of X in the criterion

- But... the parameter estimates represent changes in the criterion on a 
logarithmic scale

- The parameter estimates that can be interpreted as multiplicative effects

- This is not too difficult to understand

---

# Poisson regression

### Example

.pull-left[


```r
glm(units ~ temp, 
    data = poisson_df, 
*   family = poisson(link = "log"))
```

&lt;table class="table" style="font-size: 14px; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.495 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.015 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 166.779 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; temp &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.039 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 228.778 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]

--

.pull-right[

&lt;img src="index_files/figure-html/icecream_plot1-1.png" width="504" /&gt;

]

--

.footnote[
- A 1 unit change in temperature is associated with a  
change of 0.04 log units in ice cream sold. 

- We can exponentiate 0.04 to make it more  
interpretable.  `exp(coef(mod_poisson)[2])` = 
0.04

- A 1 unit change in temperature gives a 4% positive  
increase in ice cream sold.
]

---

# Poisson regression

### Example

.pull-left[


```r
*glm(units ~ temp + city,
    data = poisson_df, 
    family = poisson(link = "log"))
```

&lt;table class="table" style="font-size: 14px; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.450 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.016 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 157.247 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; temp &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.043 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 232.816 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; cityTucson &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.542 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.007 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -82.860 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]

--

.pull-right[

&lt;img src="index_files/figure-html/icecream_plot2-1.png" width="504" /&gt;

]

--

.footnote[
- A 1 unit change in temperature is associated with a  
positive difference of approx. 4% in ice cream sold  
in NYC. 

- When temp = 0, ice cream sold in Tucson is approx.  
58% less

]

---

# Poisson regression

### Example

.pull-left[


```r
poisson_df %&gt;% 
  mutate(., temp_c = temp - mean(temp)) %&gt;% 
* glm(units ~ temp_c + city,
      data = ., 
      family = poisson(link = "log"))
```

&lt;table class="table" style="font-size: 14px; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.411 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.005 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1130.126 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; temp_c &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.043 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 232.816 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; cityTucson &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.542 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.007 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -82.860 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]

--

.pull-right[

&lt;img src="index_files/figure-html/icecream_plot3-1.png" width="504" /&gt;

]

--

.footnote[
- A 1 unit change in temperature is associated with a  
positive difference of approx. 4% in ice cream sold  
in NYC. 

- At the average temperature (68.5), ice cream  
sold in Tucson is (still) approx. 58% less

]

---

# Poisson regression

### Summary

- Poisson regression is a powerful tool for modeling count data

- The `glm()` function works similarly to the `lm()` function 

- We test for main effects and interactions the same way too, i.e., using 
nested model comparisons with the `anova()` function

- The exponential family and corresponding linking function are  
`family = poisson(link = "log")`

- Interpretation of poisson regression works the same way as classic linear 
regression 

- Parameter estimates are evaluated on a log (multiplicative) scale and are 
not difficult to interpret

---

&lt;iframe src="https://jvcasillas.shinyapps.io/shiny_glm/" style="border:none;" height="800" width="100%"&gt;&lt;/iframe&gt;

---

# Practice

- [Poisson regression practice](./assets/poisson_regression_walkthrough/glm_poisson.zip)









---
exclude: true

[WG16; Har99; Har99; Har99]

---
layout: false
class: title-slide-final, left

# References

Hardy, M. (1993a). "Assessing Group Differences in Effects". In:
_Regression with Dummy Variables_. Ed. by M. Hardy. Sage
University Paper Series on Quantitative Applications in the Social
Sciences - 93. Newbury Park, CA: Sage, pp. 29-63. ISBN:
9780803951280.

Hardy, M. (1993b). "Creating Dummy Variables". In: _Regression
with Dummy Variables_. Ed. by M. Hardy. Sage University Paper
Series on Quantitative Applications in the Social Sciences - 93.
Newbury Park, CA: Sage, pp. 7-17. ISBN: 9780803951280.

Hardy, M. (1993c). "Using Dummy Variables as Regressors". In:
_Regression with Dummy Variables_. Ed. by M. Hardy. Sage
University Paper Series on Quantitative Applications in the Social
Sciences - 93. Newbury Park, CA: Sage, pp. 18-28. ISBN:
9780803951280.

Wickham, H. and G. Grolemund (2016). _R for Data Science: Import,
Tidy, Transform, Visualize, and Model Data_. O'Reilly Media.

Nelder, J., Wedderburn, R. (1972). Generalized Linear Models. *Journal of the Royal Statistical Society*. 135 (3). 370–384.
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://www.jvcasillas.com/ru_xaringan/js/ru_xaringan.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "default",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
