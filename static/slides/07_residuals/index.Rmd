---
title   : 'Data Science for Linguists'
subtitle: 'Residuals'
author  : "Joseph V. Casillas, PhD"
date    : "Rutgers University</br>Spring 2018</br>Last update: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: assets
    css: ["hygge", "rutgers", "rutgers-fonts"]
    nature:
      beforeInit: ["https://www.jvcasillas.com/ru_xaringan/js/ru_xaringan.js", "https://platform.twitter.com/widgets.js"]
      highlightStyle: default
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../assets/partials/header.html"
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r eval=FALSE, echo=FALSE}
rmarkdown::render("./static/slides/07_residuals/index.Rmd")
xaringan::inf_mr()
```

```{r, 'helpers', echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE}
library(knitr)
source('../assets/scripts/helpers.R')
source('./assets/scripts/lmem.R')
```

```{r, load_refs, echo=FALSE, cache=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE, 
           bib.style = "authoryear", 
           cite.style = 'alphabetic', 
           style = "markdown",
           hyperlink = FALSE, 
           dashed = FALSE)
bib <- ReadBib("../../readings/files/span589.bib", check = FALSE)
```

```{r global_setup, echo=FALSE}
opts_chunk$set(fig.retina=2, cache=TRUE)
```

class: title-slide-section-grey, middle


# .white[What is Data Science again?]

---
background-image: url("../../sources/img/workflow/data_science1.png")
background-size: contain

---
background-image: url("../../sources/img/workflow/data_science2.png")
background-size: contain

---
background-image: url("../../sources/img/workflow/data_science3.png")
background-size: contain

---
background-image: url("../../sources/img/workflow/datascience_workflow1.png")
background-size: contain

--

.center[.big[**You have learned how to version control this process!**]]

---
background-image: url("../../sources/img/workflow/datascience_workflow2.png"), url(../../sources/img/workflow/workflow_10.png)
background-size: 600px, 700px
background-position: 15% 90%, 95% 40%

--

.center[.big[**You have learned how to version control this process!**]]
---
class: title-slide-section-grey, middle

# .white[So what was version control  
again?]

--
background-image: url("../../sources/img/workflow/final_doc.png")
background-size: 525px
background-position: 100% 50%

---
background-image: url("../../sources/img/workflow/datascience_workflow3.png")
background-size: 600px
background-position: 10% 50%

# Don't forget the stats...

--

.pull-right[

```{r, warning=FALSE, message=FALSE, fig.retina=2, fig.height=5}
library(tidyverse)
ggplot(mtcars, aes(x = disp, y = mpg)) + 
  geom_point() + 
  {{geom_smooth(method = 'lm')}}
```

]

---
background-image: url("../../sources/img/workflow/datascience_workflow4.png")
background-size: 500px
background-position: 90% 50%

# We do literate programming

.pull-left[

- This means we write code in a way that clearly documents 
what we did.

- Instead of writing code with the purpose of telling the 
computer what to do, we write code that tells other humans 
what we told the computer to do and *why*. 

- Importantly, we don't separate our code from the report/essay/manuscript
we are writing. Everyting is together, in a single document.  

]

---
class: center, middle

# .black[In this class you have learned to...]

--

### .big[.RUred[manage version controlled research projects]] in a way that facilitates collaboration and honesty

--

### get and .big[.blue[tidy]] data

--

### transform and .big[.RUred[visualize]] your data

--

### fit statistical .big[.blue[models]] to your data and test hypotheses

--

### .big[.black[communicate]] your results using .big[.RUred[literate programming]]

---
class: center, middle

## This is .big[**reproducible research**]

---

class: title-slide-section-grey, middle

# Next steps

---
layout: true

# What we've seen

- MRC

- Linear regression

- General linear model

- Generalized linear model

---

background-image: url(./assets/img/lm_ex1.png)
background-position: 90% 50%
background-size: 750px

---

background-image: url(./assets/img/lm_ex2.png)
background-position: 90% 50%
background-size: 750px

---
layout: false

# What about repeated measures designs?

- Whenever we have more than one data point from the same 
participant we are dealing with a type of repeated measures 
design
  - between subjects factor
  - within subjects factor

<p></p>

- Everything we have done this semester has been under the 
assumption that we have one data point per participant (i.e., 
no within subjects factors)

- This is because one of the assumptions of our models was that 
there was no autocorrelation, i.e., that the data were independent 

- Repeated measures designs introduce autocorrelation into the 
model. Why?

---

# What's the big deal?

- Disregarding lack of independence = pseudo-replication

- In other words, replicating the data as though they were 
independent when they arenâ€™t

- This will inflate your degrees of freedom, completely bias your 
parameter estimates and make your p-values meaningless

---

# What does a repeated measures design look like?

.pull-left[
- This dataset has the weight of 50 different chicks at 12 
different time points

- Notice that the first 12 rows come from the same `chick` (chick 
1)

- In other words, `Time` can be thought of as a continuous time series 
variable (within-subjects) that would violate our assumptions of independence

- `Diet`, on the other hand, is a between-subjects factor (you 
can't be on more than one diet at a time)

- Notice that both `Time` and `Diet` appear in this dataset as 
numeric values. 

- You have to *think* about your data to make sure you understand 
what type of variables you have.
]

.pull-right[
```{r, 'chicks', cache=FALSE}
head(ChickWeight, 20)
```
]

---

# What can we do?

- Currently, the most popular solution is to use a multi-level or 
hierarchical model

- These models are commonly referred to as mixed effects models 

- They include both fixed effects and random effects

- They provide a flexible framework that allows us to account for 
the hierarchical structure of our data (within-subjects variables, 
time-series data, etc.)

- Building on our knowledge of the linear model, understanding 
the basic idea of mixed effects models is trivial

---

# How does it work? 

### Standard linear model

- Recall our formula for fitting a linear model

--

.Large[$$\hat{y} = \alpha + \beta X + \epsilon$$]

--

.Large[$$response = intercept + slope * predictor + error$$]

--

- We fit these models in R using `lm()` or `glm()`  
##.center[
```
lm(criterion ~ predictor, data = my_data)
```
]

- In a mixed effects model, what we know as *predictors* are 
called **fixed effects**

--

- The novel aspect of a mixed effects model is that it also 
includes .blue[random effects]

- Random effects allow us to account for non-independence

---

# How does it work? 

### Mixed effects model

- A mixed effects model *mixes* both fixed effects and random effects

--

.Large[$$\hat{y} = \alpha + \color{red}{\beta}\color{blue}{X} + \color{green}{u}\color{purple}{Z} + \epsilon $$]

--

.Large[$$response = intercept + \color{red}{slope} \times \color{blue}{FE} + \color{green}{u} \times \color{purple}{RE} + error$$]

--

- We fit these models in R using `lmer()` or `glmer()` from the `lme4` package (there are other options as well)

.center[
```
lmer(criterion ~ fixed_effect + (1|random_effect), data = my_data)
```
]

---

# What is a random effect? 

- This is actually a really nuanced question and nobody has a good 
answer

- Some people use descriptions like the following:

| fixed                     | random                 |
| :------------------------ | :--------------------- |
| repeatable                | non repeatable         |
| systematic influence      | random influence       |
| exhaust the pop.          | sample the pop.        |
| generally of interest     | often not of interest  |
| continuous or categorical | have to be categorical |

- ...but you will always find exceptions to this

- Most of the time you will see subjects and items as random effects 
(things that are repeated in the design)

- This typically implies that the model includes **random intercepts**, 
and/or .blue[random slopes]

- Instead of trying to define random effects, let's try to 
understand what they do

---

```{r, 'lmem_plot_raw', cache=FALSE, fig.width=14, fig.height=8, echo=FALSE}
ggplot(my_df, aes(x = time, y = response)) + 
  geom_point(pch = 21, fill = 'grey60', size = 3) + 
  my_theme()
```

---
class: middle

.pull-left[
```{r, 'lmem_structure', cache=FALSE, echo=FALSE}
select(my_df, subjects, time, response) %>% 
  as.data.frame(.) %>% 
  head(24) %>% 
  kable(., format = 'html')
```
]

--

</br></br></br></br>

.pull-right[
- The dataframe includes values of the `response` variable at 20 
different time points for each participant (n = 10)

- We could model this as:  
```
lm(response ~ time, data = my_df)
```

- But this would violate our assumption of independence (and would 
produce a terribly misspecified model)
]

---

```{r, 'lmem_plot_lm', cache=FALSE, fig.width=14, fig.height=8, echo=FALSE}
ggplot(my_df, aes(x = time, y = response)) + 
  geom_point(pch = 21, fill = 'grey60', size = 3) + 
  geom_smooth(method = lm) + 
  my_theme()
```

---

# Let's include `subjects` as a random effect

- We will do this by giving subjects a random intercept

- In other words, we will allow the intercepts to vary for each 
participant (as opposed to modeling just 1 intercept)

.center[
```
lmer(response ~ time + (1|subjects), data = my_df)
```
]

- What would this look like?

---

```{r, 'lmem_plot_subj', cache=FALSE, fig.width=14, fig.height=8, echo=FALSE}
ggplot(my_df, aes(x = condition, y = response, fill = subjects)) + 
  geom_point(pch = 21, color = 'grey60', size = 3) + 
  my_theme()
```

---

```{r, 'lmem_plot_ranInt', cache=FALSE, fig.width=14, fig.height=8, echo=FALSE}
ggplot(my_df, aes(x = condition, y = response, fill = subjects)) + 
  geom_point(pch = 21, color = 'grey60', size = 3) + 
  geom_abline(data = ran_ints, size = 1, show.legend = F, 
              color = 'grey60', 
              aes(intercept = intercepts, slope = slopes)) + 
  geom_abline(intercept = fixef(mod_int)[1], 
              slope = fixef(mod_int)[2], 
              color = 'darkred', size = 2) + 
  my_theme()
```

---
background-image: url(../assets/img/pensar2.png)
background-position: 90% 50%

# What did we do?

.pull-left[
- We are taking into account the idiosyncratic differences 
associated with each individual subject

- By giving each subject its own intercept we are informing the 
model that each individual has a different starting point in 
the time course (when `time` = 0)

<p></p>

- In general this makes sense because some people...
  - are faster/slower responders
  - speak faster/slower
  - have higher/lower pitched voices

<p></p>

- We can also take into account the fact that some people...
  - slow down during an experiment
  - respond more/less accurately over time
  - learn at different rates
]

---

```{r, 'lmem_plot_ranSlopes', cache=FALSE, fig.width=14, fig.height=8, echo=FALSE}
ggplot(my_df, aes(x = condition, y = response, fill = subjects)) + 
  geom_point(pch = 21, color = 'grey60', size = 3) + 
  geom_smooth(aes(color = subjects), method = lm, se = F) + 
  my_theme()

```

---
background-image: url(../assets/img/pensar2.png)
background-position: 90% 50%

# What did we do?

.pull-left[
- The model now allows the random intercepts to vary for each 
individual for the effect `time`

- This means we included a random slope for each participant

- By adding a random slope for the effect `time` we take into 
account the fact that `response` change for each individual at 
a different rate

- Under the hood, the model uses this information to calculate 
the best fit line for all of the data

- This method is called partial pooling and represents one of 
the most important (and least understood) aspects of mixed 
effects modeling
]

---

.pull-left[
</br></br></br></br>

```
lmer(response ~ time + (1 + time|subjects), data = my_df)
```

- Again, `(1 + time|subjects)` represents the random structure 
of the model

- Anything to the right of `|` is a random intercept

- Anything to the left of `|` is given a random slope for the 
effect specified to the right

- Thus, `(1 + time|subjects)` means random slopes for the effect `time` for each subject

- It is rarely a good idea to only use random intercepts
]

.pull-right[
```{r, 'lmem_plot_ranSlopes2', cache=FALSE, fig.width=7, fig.height=9, echo=FALSE}
ggplot(my_df, aes(x = condition, y = response, group = subjects)) + 
  geom_point(aes(color = subjects), pch = 21, fill = 'grey60', size = 3, show.legend = F) + 
  geom_smooth(method = lm, se = F, color = 'grey60') + 
  geom_abline(intercept = fixef(mod_slp)[1], 
              slope = fixef(mod_slp)[2], 
              color = 'darkred', size = 2) + 
  my_theme()
```
]

---

# Great news!

- We have spent the entire semester building up our knowledge 
of the linear model so that we would be prepared to understand 
mixed effects models 

- Why? They are the standard in social sciences now

--

<p></p>

- Everything that you have learned in this class applies 
to a mixed effects model 
  - model interpretation
  - nested model comparisons
  - treatment of categorical factors
  - centering, standardizing, and other transformations

--

<p></p>

- What about GLMs?
  - Them too!
  - You can fit mixed effects models for count data and 
  binary outcomes using `glmer()`
  - All you have to do is specify the distribution and the 
  linking function

--

```
glmer(response ~ fixed_effect + (1 + fixed_effect|participant), family = binomial(link = "logit"))
```

```
glmer(counts   ~ fixed_effect + (1 + fixed_effect|participant), family = poisson(link = "log"))
```

---

background-image: url(./assets/img/lm_ex4.png)
background-size: contain

---
background-image: url(./assets/img/gelman_hill.png)
background-position: 90% 50%

# Moving forward

.pull-left[
- Unfortunately we don't have enough time to go into more detail

- Your journey with programming and statistics is just getting started

- Use your knowledge to think critically about what you read and about your own data

- New techniques and methods are constantly coming out, but it 
seems unlikely we will stray too far from the linear model

<p></p>

- Book recommendations
  - Gelman & Hill (2007)
  - McElreath (2015)
]

---
background-image: url(https://images.tandf.co.uk/common/jackets/amazon/978148225/9781482253443.jpg)
background-position: 90% 50%

# Moving forward

.pull-left[

- Getting help
  - stackoverflow.com
  - coursera.con
  - datacamp.com

<p></p>

- Coding (things to learn)
  - functional programming
  - leaving the tidyverse...

<p></p>

- Useful links for mixed effects models
  - [Getting started](http://www.bodowinter.com/tutorial/bw_LME_tutorial1.pdf)
  - [Intro](https://ourcodingclub.github.io/2017/03/15/mixed-models.html)
  - [Conceptual understanding](http://mfviz.com/hierarchical-models/)
  - [Logistic regression](http://www.karlin.mff.cuni.cz/~pesta/NMFM404/mixed.html)
  - [Specifying random effects](http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#model-specification)
  - [More links](https://socialbyselection.wordpress.com/2015/07/14/mixed-effect-models/)

]

---
exclude: true

`r AutoCite(bib, c("wickham2016r", "qass93_ch2", "qass93_ch3", "qass93_ch4"))`

---
layout: false
class: title-slide-final
background-image: url(https://github.com/jvcasillas/ru_xaringan/raw/master/img/logo/ru_shield.png)
background-size: 55px
background-position: 9% 15%

</br></br></br>

# Data Science for Linguists

## www.jvcasillas.com/ru_teaching/ru_spanish_589/589_01_s2018/


</br>

|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="mailto:joseph.casillas@rutgers.edu">.RUred[<i class="fa fa-paper-plane fa-fw"></i>]               | joseph.casillas@rutgers.edu       |
| <a href="http://twitter.com/jvcasill">.RUred[<i class="fa fa-twitter fa-fw"></i>]                          | @jvcasill                         |
| <a href="http://github.com/jvcasillas">.RUred[<i class="fa fa-github fa-fw"></i>]                          | @jvcasillas                       |

# Thanks!
