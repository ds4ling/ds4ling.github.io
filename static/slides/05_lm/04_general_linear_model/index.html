<!DOCTYPE html>
<html>
  <head>
    <title>Data Science for Linguists</title>
    <meta charset="utf-8">
    <meta name="author" content="Joseph V. Casillas, PhD" />
    <link href="assets/remark-css/hygge.css" rel="stylesheet" />
    <link href="assets/remark-css/rutgers.css" rel="stylesheet" />
    <link href="assets/remark-css/rutgers-fonts.css" rel="stylesheet" />
    <script src="https://use.fontawesome.com/5235085b15.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Data Science for Linguists
## The general linear model
### Joseph V. Casillas, PhD
### Rutgers University</br>Spring 2018</br>Last update: 2018-03-20

---













class: inverse, middle

&lt;blockquote align='center' class="twitter-tweet" data-lang="de"&gt;
&lt;a href="https://twitter.com/tres_gonzaga/status/910822470927523840"&gt;&lt;/a&gt;
&lt;/blockquote&gt;

---
class: title-slide-section-grey, middle

# The general linear model

---
layout: true

# A quick review

---

### Classical MRC

- In classical Multiple Regression/Correlation (MRC) predictors are 
continuous variables

&lt;p&gt;&lt;/p&gt;

- Recall
  - Darwinian theory predicted continuous variation in traits
  - Galton and Pearson created a model in which continuous variables were used
  - Discontinuous (categorical) variables were not considered

&lt;p&gt;&lt;/p&gt;

- In real life we do run into dichotomous/discontinuous variables

- Or... if an entire experimental group gets one treatment and another group 
gets a different treatment, this is also dichotomous/discontinuous

---

### Classical ANOVA

- Classical Analysis of Variance (ANOVA) assumes that all predictors are 
discontinuous variables

- ANOVA methods are often abused by forcing continuous variables into 
discontinuous form (this can reduce your statistical power by as much as 50%)

---

### Both types of variables (continuous and categorical) exist in the real world

.pull-left[

### Categorical

- Biological sex (Male/Female)

- Smoker/Non-smoker

- Native speaker/L2 learner

- Stressed/Unstressed syllable

- Etc.

]

.pull-right[

### Continuous

- Age

- Weight

- Amount of exercise

- Miles per gallon

- VOT

]

---

### The modern GLM

#### Modern GLM includes both MRC and ANOVA:

- MRC predictors are all continuous

- ANOVA predictors are all discontinuous/categorical

- But both MRC and ANOVA are both part of the same big thing: the GLM

---

### A unified model

- Before ANOVA and MRC were unified, we could not account for both levels of 
measurement within the same model

- The modern GLM can accommodate any combination of categorical and continuous 
variables

- This was not known until 1968 (Cohen, 1968)

- So now we can construct mixed models with both categorical and continuous 
variables

---
layout: true

# A brief history

---

### Remember...

.pull-left[

#### Aristotle

- Categories are inherent in the individual
- Individuals need to have some identifiable, visible feature to be classified
- It is the possession of certain observable features that puts individuals in 
categories

]

--

.pull-right[

#### Plato

- Categories are God-given
- Things possess an essence of a type
- Observable features are not reliable because they are based on sense 
perceptions

]

--

&lt;/br&gt;

#### .RUred[To Aristotle the individual was ultimate reality, but to Plato the individual was an imperfect reflection of the perfect category or its “ideal type” (= eidolon)]

---
background-image: url(https://theancientwebgreece.files.wordpress.com/2017/09/bc284-bob5_just2.jpg?w=337&amp;zoom=2)
background-size: contain
background-position: 100%

### How unification occurred

.pull-left[

- Platonic and Aristotelian philosophy came back into European culture 
(the Essentialists and the Nominalists in Scholastic Philosophy)

- Nominalists assert that groups are mental constructs (not like Plato's 
god-given groups)

- This kind of Aristotelian reasoning was revived in statistics by Jacob Cohen 
in 1968

- The solution to unifying MRC and ANOVA: .RUred[Dummy Variable Coding]

]

---

### The two disciplines

Lee Cronbach (1957) *Two Disciplines of Scientific Psychology*

.pull-left[

#### Differential Psychology&lt;/br&gt;(Galton and Pearson) 

- Influenced by Darwinian thinking

- Based on individual differences

- This led to the development of MRC

]

--

.pull-right[

#### Experimental Psychology&lt;/br&gt;(Fechner, Weber, and Wundt) 

- Relied more on typological approach

- Categorically distinct groupings to design and carry out experiments

- This led to the development of ANOVA (Fisher)

]

---

### The two disciplines

#### MRC and ANOVA are both part of the same GLM model but, over time, they began to diverge from each other:

- We need them both, but historically this separation occurred

#### Fisher (experimentalists) versus Pearson (observationalists):

- A personal/family feud existed between them
- Pearson was principle figure in stats, criticized Fisher's use of chi-square 
test in an old paper
- Thought that Fisher had done a disservice to statistics (see Lenhard, 2006)
- Both angry, held grudges

---
background-image: url(https://geneonline.news/wp-content/uploads/2016/02/Ronald-Fisher-from-Royal-Society-e1455600445283-1024x850.jpg)
background-size: 400px
background-position: 90%

### Sir Ronald Aylmer Fisher

.pull-left[

- Did agricultural experiments with plants at Rothamsted Experimental Station at Harpenden, Hertfordshire, England (Studies in Crop Variation, 1919)

- Actually did controlled experiments

- Unlike the other differential psychologists who just did observational studies

- Did not have a Platonist ideology and understood individual differences

]

---
background-image: url(https://geneonline.news/wp-content/uploads/2016/02/Ronald-Fisher-from-Royal-Society-e1455600445283-1024x850.jpg)
background-size: 400px
background-position: 90%

### Sir Ronald Aylmer Fisher

#### Invented ANOVA to support the experimental method:

.pull-left[

- Randomly selected groups will differ by some amount due to individual 
differences among members of the group

- However, experimental groups should be different beyond these random 
individual differences

- He didn’t want to just assume the groups were different

- He wanted to show the variance between groups was greater than the variance 
within groups

]

---

### Fisher's method

- Fisher's used random assignment, not random sampling 

- Random Sampling: any individual in the population has an equal chance of 
being in the sample

- Random Assignment:
  - You randomly assign each subject to a treatment group or control group
  - You create 2 or more groups that will be subjected to 2 or more different 
  treatments
  - This is important to show that differences between treatment groups is 
  greater than chance

--

- Your sample might not be random, but your assignment to experimental groups 
should be random
- This procedure has nothing to do with representing the original population:
  - Just with how you randomly assign individuals into treatment groups from 
  the basic sample you are working with
  - Each subject must have an equal chance to get into either of the treatment 
  groups

---

### Fisher's method

- Using the central limit theorem, we know how the distribution of randomly 
selected group means will differ from the mean of the entire population

- If the treatment worked, the results should be greater than what would be 
expected by chance (meaning sampling of individuals)

- If treatment did not work, the results would not be greater than what is 
expected by chance (meaning sampling of individuals)

--

- This is what the F-ratio (for Fisher) is all about

- The numerator in the F-ratio represents the variance due to treatment effects

- The denominator is an independent estimate of the random sampling "error"

- All ANOVA assumes random assignment not necessarily random sampling

---

### Fisher's method

- This was designed for purely experimental purposes, not for naturally 
occurring phenomena, i.e., observational studies

- Should not use classical ANOVA for observational studies, only for pure 
controlled experiments

- For example, one should not use ANOVA to study naturally occurring races, 
sexes, etc., because of lack of random assignment to these groups

- For example, if you compare males with females, this is not a randomly assigned condition

--

- Researchers do this anyway

---

### Analysis of Variance (ANOVA)

- How did Sir Ronald Fisher build the ANOVA model?

- He built it from the MRC model...

---

### Summed Linear Deviations (MRC)

Sum of Linear Deviations:  

.Large[
`$$(y_{i} - \bar{y}) = (\hat{y}_{i} - \bar{y}) + (y_{i} - \hat{y}_{i})$$`
]

--

.center[
### Total Deviation = Predicted Deviation + Error Deviation
]

---

### Multiple Regression/Correlation

#### In MRC, the predicted y ( `\(\hat{y}_{i}\)` ) is the score predicted based on the regression line:

- MRC is based on having individual scores as the criterion variable (y)

- Individual continuous variables are also the predictors for y

---

### Summed Linear Deviations (ANOVA)

Sum of Linear Deviations:  

.Large[
`$$(y_{i} - \bar{y}_{G}) = (\bar{y}_{j} - \bar{y}_{G}) + (y_{i} - \bar{y}_{j})$$`
]

.center[

### Total Deviation = Predicted Deviation + Error Deviation

]

---

### Analysis of Variance

In ANOVA, you are dealing with groups:

- Still trying to predict an individual's score

- But you aren't basing your prediction on other individual scores

- You are basing it on their group status

---

### Analysis of Variance

#### What is the best prediction you can make about any individual in a group if you don’t know anything else about that individual?

- Use the mean of the group to predict individual scores

- Our predicted score ( `\(\hat{y}_{i}\)` ) now becomes our group mean ( `\(\bar{y}_{j}\)` )

- So the group mean ( `\(\bar{y}_{j}\)` ) now becomes the predicted `\(\hat{y}_{i}\)`

--

#### This is because my prediction for you (if I don’t know anything else about you) is based on your group’s mean

- The grand mean is `\(\bar{y}_{G}\)` and the group mean is `\(\bar{y}_{j}\)`

---

### Sums of Squared Deviations (MRC)

SS = Sum of squares: 

.Large[
`$$\sum (y_{i} - \bar{y})^2 = \sum (\hat{y}_{i} - \bar{y})^2 + \sum (y_{i} - \hat{y}_{i})^2$$`
]

--

.Large[
`$$SS_{Total} = SS_{Predicted} + SS_{Error}$$`
]

--

.Large[
|                  |     |                                  |
| :--------------- | :-: | :------------------------------- |
| `\(SS_{Total}\)`     |  =  | `\(\sum (y_{i} - \bar{y})^2\)`       |
| `\(SS_{Predicted}\)` |  =  | `\(\sum (\hat{y}_{i} - \bar{y})^2\)` |
| `\(SS_{Error}\)`     |  =  | `\(\sum (y_{i} - \hat{y}_{i})^2\)`   |
]

---

### Sums of Squared Deviations (ANOVA)

SS = Sum of squares: 

.Large[
`$$\sum (y_{i} - \bar{y}_{G})^2 = \sum (\bar{y}_{j} - \bar{y}_{G})^2 + \sum (y_{i} - \bar{y}_{j})^2$$`
]

--

.Large[
`$$SS_{Total} = SS_{Predicted} + SS_{Error}$$`
]

--

.Large[
|                  |     |                                      |
| :--------------- | :-: | :----------------------------------- |
| `\(SS_{Total}\)`     |  =  | `\(\sum (y_{i} - \bar{y}_{G})^2\)`       |
| `\(SS_{Predicted}\)` |  =  | `\(\sum (\bar{y}_{j} - \bar{y}_{G})^2\)` |
| `\(SS_{Error}\)`     |  =  | `\(\sum (y_{i} - \bar{y}_{j})^2\)`       |
]

---

### Squared Multiple Correlation Coefficient (MRC)

.Large[
`$$R^2 = \frac{\sum (\hat{y}_{i} - \bar{y})^2} {\sum (y_{i} - \bar{y})^2}$$`

`$$R^2 = \frac{SS_{Predicted}} {SS_{Total}}$$`
]

Coefficient of determination  
Proportion of Variance Explained

---

### Squared Multiple Correlation Coefficient (ANOVA)

.Large[
`$$R^2 = \frac{\sum (\hat{y}_{j} - \bar{y}_{G})^2} {\sum (y_{i} - \bar{y}_{G})^2}$$`

`$$R^2 = \frac{SS_{Predicted}} {SS_{Total}}$$`
]

Coefficient of determination  
Proportion of Variance Explained

---

### Mean Squared Deviations (MRC)

MS = Mean Squares (Variances):

.Large[
|                  |     |                                                     |
| :--------------- | :-: | :-------------------------------------------------- |
| `\(MS_{Total}\)`     |  =  | `\(\frac{\sum (y_{i} - \bar{y})^2} {(n - 1)}\)`         |
| `\(MS_{Predicted}\)` |  =  | `\(\frac{\sum (\hat{y}_{i} - \bar{y})^2} {(k)}\)`       |
| `\(MS_{Error}\)`     |  =  | `\(\frac{\sum (y_{i} - \hat{y}_{i})^2} {(n - k - 1)}\)` |

`$$F_{(k), (n-k-1)} = \frac{\sum (\hat{y}_{i} - \bar{y})^2 / (k)}
                          {\sum (y_{i} - \hat{y}_{i})^2 / (n - k - 1)}$$`
]

---

### Mean Squared Deviations (ANOVA)

MS = Mean Squares (Variances):

.Large[
|                  |     |                                                |
| :--------------- | :-: | :--------------------------------------------- |
| `\(MS_{Total}\)`     |  =  | `\(\sum (y_{i} - \bar{y}_{G})^2 / (n - 1)\)`       |
| `\(MS_{Predicted}\)` |  =  | `\(\sum (\bar{y}_{j} - \bar{y}_{G})^2 / (g - 1)\)` |
| `\(MS_{Error}\)`     |  =  | `\(\sum (y_{i} - \bar{y}_{j})^2 / (n - g)\)`       |

`$$F_{(g-1), (n-g)} = \frac{\sum (\bar{y}_{j} - \bar{y}_{G})^2 / (g - 1)}
                          {\sum (y_{i} - \hat{y}_{j})^2 / (n - g)}$$`
]

---

### Mean Squared Deviations (MRC/ANOVA)

MS = Mean Squares (Variances):

.Large[
|                  |     |                                   |
| :--------------- | :-: | :-------------------------------- |
| `\(MS_{Total}\)`     |  =  | `\(SS_{Total} / df_{Total}\)`         |
| `\(MS_{Predicted}\)` |  =  | `\(SS_{Predicted} / df_{Predicted}\)` |
| `\(MS_{Error}\)`     |  =  | `\(SS_{Error} / df_{Error}\)`         |

`$$F-ratio = \frac{MS_{Predicted}} {MS_{Error}}$$`
]

---

### Degrees of Freedom

.pull-left[

### MRC

.Large[
|                  |     |           |
| :--------------- | :-: | :-------- |
| `\(df_{Total}\)`     |  =  | n - 1     |
| `\(df_{Predicted}\)` |  =  | k         |
| `\(df_{Error}\)`     |  =  | n - k - 1 |

`$$df_{Total} = df_{Predicted} + df_{Error}$$`
]
]

.pull-right[

### ANOVA

.Large[
|                  |     |       |
| :--------------- | :-: | :---- |
| `\(df_{Total}\)`     |  =  | n - 1 |
| `\(df_{Predicted}\)` |  =  | g - 1 |
| `\(df_{Error}\)`     |  =  | n - g |

`$$df_{Total} = df_{Predicted} + df_{Error}$$`
]
]

---

### Equivalences

.Large[
| MRC              |     | ANOVA      |
| :--------------- | :-: | :--------- |
| .white[.]        |     |            |
| `\(SS_{Predicted}\)` |  =  | `\(SS_{BG}\)`  |
| `\(SS_{Error}\)`     |  =  | `\(SS_{WG}\)`  |
| .white[.]        |     |            |
| `\(MS_{Predicted}\)` |  =  | `\(MS_{BG}\)`  |
| `\(MS_{Error}\)`     |  =  |  `\(MS_{WG}\)` |
| .white[.]        |     |            |
| k                |  =  | g - 1      |
| n - k - 1        |  =  | n - g      |
]

---

### Equivalences (MRC/ANOVA)

- ANOVA is just MRC where the predictors are categorical variables

--

- Predictions are based entirely on the group status of individuals

--

- The criterion variables are still in continuous

--

&lt;p&gt;&lt;/p&gt;

- In a controlled experiment, these are orthogonal variables:
  - Conclusions may not be theoretically valid if you are using naturally 
  occurring groups
  - But the math will still work

&lt;p&gt;&lt;/p&gt;

- The strength of ANOVA is based on proper experimental design

---

### The Logic of the F-Ratio

- If we use random assignment, the means of the groups should only differ 
by random chance (individual differences) and nothing else

- However, if we introduce an effective treatment, then the means will differ 
by random chance *plus* the effects of the treatment

- But if the treatment didn’t work, then the means will only differ by random 
chance

- This is the denominator of the F-ratio!

---
background-image: url(https://memegenerator.net/img/instances/81519428/hey-girl-whats-your-number.jpg), url(https://memegenerator.net/img/instances/81519424/hey-girl-whats-your-type.jpg)
background-position: 20% 60%, 83% 60%
background-size: 300px, 300px

.pull-left[

### .center[MRC]

]

.pull-right[

### .center[ANOVA]

]

--

.footnote[So how did unification occur?]

---
layout: true

# The General Linear Model

---
background-image: url(https://jamanetwork.com/data/Journals/PSYCH/926697/m_yot8401f1.png)
background-position: 90%
background-size: 300px

### Jacob Cohen (1923-1998)

- Statistician and Psychologist
  - Best known for his work on statistical power and effect size
  - Helped lay foundations for meta-analysis
  - Gave his name to Cohen's kappa and Cohen's d
- Made a breakthrough in how to create the modern GLM:
  - Introduced Dummy Variable method of coding groups
  - Makes MRC do ANOVA!

&lt;p&gt;&lt;/p&gt;

- (1968) Multiple regression as a general data analytic system
- (1969) Statistical Power Analysis for the Behavioral Sciences
- (1975) Applied Multiple Regression/Correlation Analysis for  
the Behavioral Sciences
- (1990) Things I have learned (so far) (1994) The earth is  
round (p&lt;.05)

---

### Dummy variables

- This was based upon the idea that category membership can be considered an 
individual characteristic

&lt;p&gt;&lt;/p&gt;

- Your category membership is part of who you are
  - You may have other individual differences in addition to your category 
  membership
  - But membership is also an individual trait!

&lt;p&gt;&lt;/p&gt;

- This was Aristotelian/Nominalist, not Platonic/Essentialist typological 
thinking!

- Aristotle said that categories inhere in the individual and that the 
individual is the ultimate reality

---

### Dummy variables

- In a sense Cohen took an Aristotelian approach in order to include both MRC 
and ANOVA within one *general* linear model

- You score each person by a dummy variable to say whether they have a feature 
that defines their group membership or not

- Dummy variable coding is just binary coding of whether you have or do not 
have trait

- Dummy variable coding will make a regular regression equation do an ANOVA

- *Multiple Regression As A General Data Analytic Method* was the name of 
Cohen's seminal article

---

### The "GLM" Compromise

- The way he told the story (in 1968), MRC had seemingly “eaten” ANOVA:
  - MRC included ANOVA
  - ANOVA was just a “special case” of MRC

&lt;p&gt;&lt;/p&gt;

- This is mathematically correct, but it upset the ANOVA guys, so they instead 
called the whole superordinate category the GLM to make everyone happy:
  - GLM with continuous predictors = MRC
  - GLM with categorical predictors = ANOVA
- If you have all of one or all of another you can call it either classical 
ANOVA or a classical MRC
- But if not, you call it a "Mixed GLM"

&lt;p&gt;&lt;/p&gt;

- How did Jacob Cohen accomplish this feat?
  - He didn’t actually cite Aristotle
  - He just did the math...

---
layout: false
class: title-slide-section-grey, middle

# Dummy variables

---
layout: true

# Dummy variables

---

### The Multiple Regression Equation

.Large[
`$$\hat{y}_{i} = a + b_{1}x_{1} + b_{2}x_{2} + b_{3}x_{3} ...$$`
]

--

.Large[

What if the predictors are dummy variables?

.center[
|         |     |         |     |        |
| :-----: | :-: | :-----: | :-: | :----: |
| `\(x_{1}\)` |  =  | `\(d_{1}\)` |  =  | 0 or 1 |
| `\(x_{2}\)` |  =  | `\(d_{2}\)` |  =  | 0 or 1 |
| `\(x_{3}\)` |  =  | `\(d_{3}\)` |  =  | 0 or 1 |
]
]

---

### The Dummy Variable Equation

.large[
A single dummy variable: 
]

.Large[
`$$\hat{y}_{i} = a + b_{1}d_{1}$$`
]

--

.large[

Evaluating the function: 

|                       |             |
| :-------------------: | :---------- |
| `\(\hat{y}_{(d1=1)} =\)`  | `\(a + b_{1}\)` |
| .white[.]             |             |
| `\(\hat{y}_{(d1=0)} =\)`  | `\(a\)`         |
]

---

### Evaluating the function

.Large[
|                                                   |
| :------------------------------------------------ |
| `\(\bar{y}_{(d1=1)} = \hat{y}_{(d1=1)} = a + b_{1}\)` |
| .white[.]                                         |
| `\(\bar{y}_{(d1=0)} = \hat{y}_{(d1=0)} = a\)`         |
]

--

### Which implies that...

.Large[
`$$b_{1} = (\bar{y}_{(d1=1)} - \bar{y}_{(d1=0)})$$`
]

---

.Large[

### You get j-1 dummies

| Groups | d&lt;sub&gt;1&lt;/sub&gt; | d&lt;sub&gt;2&lt;/sub&gt; | d&lt;sub&gt;3&lt;/sub&gt; |
| :----- | :-----------: | :-----------: | :-----------: |
| A      | 0             | 0             | 0             |
| B      | 1             | 0             | 0             |
| C      | 0             | 1             | 0             |
| D      | 0             | 0             | 1             |

]

---

### One level is taken as the *reference* or *baseline*. This is the intercept.

.Large[

| Groups | d&lt;sub&gt;1&lt;/sub&gt; | d&lt;sub&gt;2&lt;/sub&gt; | d&lt;sub&gt;3&lt;/sub&gt; |                  |
| :----- | :-----------: | :-----------: | :-----------: | :--------------- |
| **A**  | **0**         | **0**         | **0**         | ⬅︎ **Intercept** |
| B      | 1             | 0             | 0             |                  |
| C      | 0             | 1             | 0             |                  |
| D      | 0             | 0             | 1             |                  |

]

---

### j-1 dummies = j-1 comparisons

.Large[

| Groups   | d&lt;sub&gt;1&lt;/sub&gt; | d&lt;sub&gt;2&lt;/sub&gt; | d&lt;sub&gt;3&lt;/sub&gt; |                  |
| :------- | :-----------: | :-----------: | :-----------: | :--------------- |
| **A**    | **0**         | **0**         | **0**         | ⬅︎ **Intercept** |
| .blue[B] | .blue[1]      | 0             | 0             |                  |
| C        | 0             | 1             | 0             |                  |
| D        | 0             | 0             | 1             |                  |
|          | ⬆︎&lt;/br&gt;**A**.blue[B]|         |               |                  |
]

---

### j-1 dummies = j-1 comparisons

.Large[

| Groups   | d&lt;sub&gt;1&lt;/sub&gt; | d&lt;sub&gt;2&lt;/sub&gt; | d&lt;sub&gt;3&lt;/sub&gt; |                  |
| :------- | :-----------: | :-----------: | :-----------: | :--------------- |
| **A**    | **0**         | **0**         | **0**         | ⬅︎ **Intercept** |
| B        | 1             | 0             | 0             |                  |
| .blue[C] | 0             | .blue[1]      | 0             |                  |
| D        | 0             | 0             | 1             |                  |
|          |               | ⬆︎&lt;/br&gt;**A**.blue[C]|         |                  |
]

---

### j-1 dummies = j-1 comparisons

.Large[

| Groups   | d&lt;sub&gt;1&lt;/sub&gt; | d&lt;sub&gt;2&lt;/sub&gt; | d&lt;sub&gt;3&lt;/sub&gt; |                  |
| :------- | :-----------: | :-----------: | :-----------: | :--------------- |
| **A**    | **0**         | **0**         | **0**         | ⬅︎ **Intercept** |
| B        | 1             | 0             | 0             |                  |
| C        | 0             | 1             | 0             |                  |
| .blue[D] | 0             | 0             | .blue[1]      |                  |
|          |               |               | ⬆︎&lt;/br&gt;**A**.blue[D] |           |
]

--

Let's see some examples...

---

### `mtcars`

&lt;img src="index_files/figure-html/mtcars_p1-1.png" width="1008" /&gt;

---


```r
lm(mpg ~ cyl, data = mtcars)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Intercept &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 26.6636 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.9718 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27.4373 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0e+00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 6-cyl &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -6.9208 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.5583 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -4.4411 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1e-04 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 8-cyl &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -11.5636 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.2986 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -8.9045 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0e+00 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;img src="index_files/figure-html/mtcars_p2-1.png" width="1008" /&gt;

---


```r
lm(mpg ~ cyl, data = mtcars)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Intercept &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 26.6636 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.9718 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27.4373 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0e+00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 6-cyl &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -6.9208 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.5583 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -4.4411 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1e-04 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 8-cyl &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -11.5636 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.2986 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -8.9045 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0e+00 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;img src="index_files/figure-html/mtcars_p3-1.png" width="1008" /&gt;

---


```r
lm(mpg ~ cyl, data = mtcars)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Intercept &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 26.6636 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.9718 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27.4373 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0e+00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 6-cyl &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -6.9208 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.5583 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -4.4411 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1e-04 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 8-cyl &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -11.5636 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.2986 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -8.9045 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0e+00 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;img src="index_files/figure-html/mtcars_p4-1.png" width="1008" /&gt;

---

.pull-left[

### Regression output


```r
mtcars %&gt;%
  lm(mpg ~ cyl, data = .) %&gt;% 
  summary(.)
```

&lt;table class="table table-hover" style="font-size: 18px; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Std. Error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; t.value &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Intercept &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: red;"&gt;26.66&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.97 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0e+00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: black;"&gt;-6.92&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.56 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -4.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1e-04 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: black;"&gt;-11.56&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.30 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -8.90 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0e+00 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]

--

.pull-right[

### Means


```r
mtcars %&gt;% 
  group_by(., cyl) %&gt;% 
  summarize(., mean_mpg = mean(mpg), 
               sd_mpg = sd(mpg))
```

&lt;table class="table table-hover" style="font-size: 18px; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; cyl &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; mean_mpg &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; sd_mpg &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: red;"&gt;26.66&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.51 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: black;"&gt;19.74&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.45 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: black;"&gt;15.1&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.56 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]

&lt;/br&gt;

- Each simple effect is an independent samples t-test
- The baseline is compared to the other two levels of the factor
--

- Notice that `6-cyl` is not compared to `8-cyl`
- We would have to change the baseline to make that comparison

---


```r
mtcars %&gt;%
  mutate(., cyl = as.factor(cyl),
*           cyl = fct_relevel(cyl, c('6', '4', '8'))) %&gt;%
  lm(mpg ~ cyl, data = .) %&gt;% 
  summary(.)
```

&lt;table class="table table-hover" style="font-size: 18px; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Std. Error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Intercept &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: red;"&gt;19.74&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.22 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 16.21 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; cyl4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: black;"&gt;6.92&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.56 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0001 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; cyl8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: black;"&gt;-4.64&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.49 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -3.11 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0042 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

- Now we have the 6-to-8 cyl comparison
- Notice how the slopes have changed

---

### Categorical and continuous predictors - mixed GLMs

- One of the benefits of doing ANOVA with MRC is that you can include different 
types of predictors in your model, i.e., **categorical** and .blue[continuous].

- Neither classical ANOVA nor classical MRC can handle combinations of these 
two predictors

- This is possible because of dummy coding

---
layout: false
class: middle

&lt;div class="figure"&gt;
&lt;img src="index_files/figure-html/age_vocab_raw-1.png" alt="Vocabulary size as a function of age." width="1008" /&gt;
&lt;p class="caption"&gt;Vocabulary size as a function of age.&lt;/p&gt;
&lt;/div&gt;

---
layout: false
class: middle

&lt;div class="figure"&gt;
&lt;img src="index_files/figure-html/age_vocab_bivar_reg-1.png" alt="Vocabulary size as a function of age." width="1008" /&gt;
&lt;p class="caption"&gt;Vocabulary size as a function of age.&lt;/p&gt;
&lt;/div&gt;

---
template: base

### Bivariate model 

.pull-left[

`$$vocab \sim age$$`




```
## 
## Call:
## lm(formula = vocab ~ age, data = vocab_sample)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6418.3 -2147.4     3.6  1685.7  6373.1 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -249.9      588.6  -0.425    0.672    
## age           1320.3       58.8  22.453   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2602 on 198 degrees of freedom
## Multiple R-squared:  0.718,	Adjusted R-squared:  0.7166 
## F-statistic: 504.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16
```
]

.pull-right[
&lt;img src="index_files/figure-html/age_vocab_bivar_reg2-1.png" width="468" /&gt;
]

---
template: base

### Additive model 

.pull-left[

`$$vocab \sim age + reader$$`




```
## 
## Call:
## lm(formula = vocab ~ age + reader, data = vocab_sample)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4685.0 -1244.7  -253.8  1132.2  4523.6 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    -2140.65     426.13  -5.023 1.13e-06 ***
## age             1317.73      40.61  32.452  &lt; 2e-16 ***
## readerfrequent  3754.74     254.17  14.772  &lt; 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1797 on 197 degrees of freedom
## Multiple R-squared:  0.8662,	Adjusted R-squared:  0.8649 
## F-statistic: 637.7 on 2 and 197 DF,  p-value: &lt; 2.2e-16
```
]

.pull-right[
&lt;img src="index_files/figure-html/age_additive_plot1-1.png" width="468" /&gt;
]

---

# Dummy variables

### Categorical and continuous predictors - mixed GLMs

- A mixed GLM can account for continuous and categorical predictors

- However, if the slope of two groups are different, then you must interact 
the categorical variable with the continuous one

- The interaction term constitutes a test for "homogeneity of slopes"

- These interaction terms accommodate the possible difference in slopes and 
therefore avoids a serious model misspecification

- Leaving them out would be omitting a relevant variable!

---

# Dummy variables

### Categorical and continuous predictors - mixed GLMs

- What does it mean to have an interaction between a dummy variable and a 
continuous predictor?

- Remember that with a continuous variable we get an intercept and a slope, so 
if one is interacting with a categorical variable, it means that either the 
*intercepts* or the *slopes* of both might be different for each category

- "Homogeneity of slopes" assumes that your different groups have the same 
slope for the continuous variable

---

# Dummy variables

### Categorical and continuous predictors - mixed GLMs

- The problem is that using either classical ANOVA or classical MRC (or even 
"ANCOVA", which is a combination of both) does not permit you to handle 
interactions between these types of variables

- But using Dummy or Contrast Coding Does!

- By virtue of the numerical nature of these "coded vectors", which can be 
accommodated by MRC

---
template: base

### Multiplicative model 

.pull-left[

`\(vocab \sim age + reader + age:reader\)`




```
## 
## Call:
## lm(formula = vocab ~ age * reader, data = vocab_sample)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5505.5 -1091.0  -143.1  1187.6  5159.9 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)         -181.29     573.61  -0.316    0.752    
## age                 1111.37      57.63  19.285  &lt; 2e-16 ***
## readerfrequent       211.93     774.68   0.274    0.785    
## age:readerfrequent   372.66      77.44   4.812 2.98e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1704 on 196 degrees of freedom
## Multiple R-squared:  0.8803,	Adjusted R-squared:  0.8785 
## F-statistic: 480.7 on 3 and 196 DF,  p-value: &lt; 2.2e-16
```

]

.pull-right[

&lt;img src="index_files/figure-html/age_int_plot1-1.png" width="468" /&gt;

]

---
template: base
class: middle


&lt;table style="text-align:center"&gt;&lt;tr&gt;&lt;td colspan="4" style="border-bottom: 1px solid black"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td colspan="3"&gt;&lt;em&gt;Dependent variable:&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td colspan="3" style="border-bottom: 1px solid black"&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td colspan="3"&gt;vocab&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;(1)&lt;/td&gt;&lt;td&gt;(2)&lt;/td&gt;&lt;td&gt;(3)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan="4" style="border-bottom: 1px solid black"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="text-align:left"&gt;Constant&lt;/td&gt;&lt;td&gt;-249.952&lt;/td&gt;&lt;td&gt;-2,140.647&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;-181.289&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;(588.602)&lt;/td&gt;&lt;td&gt;(426.132)&lt;/td&gt;&lt;td&gt;(573.610)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;age&lt;/td&gt;&lt;td&gt;1,320.274&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;1,317.727&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;1,111.367&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;(58.801)&lt;/td&gt;&lt;td&gt;(40.605)&lt;/td&gt;&lt;td&gt;(57.630)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;readerfrequent&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;3,754.738&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;211.934&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(254.174)&lt;/td&gt;&lt;td&gt;(774.680)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;age:readerfrequent&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;372.664&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(77.445)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan="4" style="border-bottom: 1px solid black"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="text-align:left"&gt;Observations&lt;/td&gt;&lt;td&gt;200&lt;/td&gt;&lt;td&gt;200&lt;/td&gt;&lt;td&gt;200&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.718&lt;/td&gt;&lt;td&gt;0.866&lt;/td&gt;&lt;td&gt;0.880&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;Adjusted R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.717&lt;/td&gt;&lt;td&gt;0.865&lt;/td&gt;&lt;td&gt;0.879&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;Residual Std. Error&lt;/td&gt;&lt;td&gt;2,602.150 (df = 198)&lt;/td&gt;&lt;td&gt;1,796.905 (df = 197)&lt;/td&gt;&lt;td&gt;1,703.657 (df = 196)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;F Statistic&lt;/td&gt;&lt;td&gt;504.148&lt;sup&gt;***&lt;/sup&gt; (df = 1; 198)&lt;/td&gt;&lt;td&gt;637.730&lt;sup&gt;***&lt;/sup&gt; (df = 2; 197)&lt;/td&gt;&lt;td&gt;480.686&lt;sup&gt;***&lt;/sup&gt; (df = 3; 196)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan="4" style="border-bottom: 1px solid black"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;em&gt;Note:&lt;/em&gt;&lt;/td&gt;&lt;td colspan="3" style="text-align:right"&gt;&lt;sup&gt;*&lt;/sup&gt;p&lt;0.1; &lt;sup&gt;**&lt;/sup&gt;p&lt;0.05; &lt;sup&gt;***&lt;/sup&gt;p&lt;0.01&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

---
template: base




```r
anova(vocab_age_null, vocab_age_mod, vocab_additive_mod, vocab_int_mod)
```

```
## Analysis of Variance Table
## 
## Model 1: vocab ~ 1
## Model 2: vocab ~ age
## Model 3: vocab ~ age + reader
## Model 4: vocab ~ age * reader
##   Res.Df        RSS Df  Sum of Sq        F    Pr(&gt;F)    
## 1    199 4754374715                                     
## 2    198 1340694767  1 3413679947 1176.138 &lt; 2.2e-16 ***
## 3    197  636086750  1  704608017  242.763 &lt; 2.2e-16 ***
## 4    196  568879766  1   67206985   23.155 2.977e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

--

The vocabulary data were analyzed using a general linear model. Estimated 
vocabulary size was the criterion with *age* and *reader type* 
(frequent/average) as predictors. The *reader type* factor was dummy coded with 
average readers set as the reference group. Main effects and the *age* by 
*reader type* interaction were assessed using nested model comparisons. 
Experiment-wise alpha was set at 0.05. 

There was a main effect of age (F(1) = 1176.138, p &lt; 0.001), reader type 
(F(1) = 242.76; p &lt; 0.001), as well as an age by reader type interaction 
(F(1) = 23.16; p &lt; 0.001). The model containing the interaction provided the 
best fit of the data (R&lt;sup&gt;2&lt;/sup&gt; = 0.88). Overall, vocabulary size increased as a 
function of age. However, the size of the effect was modulated by reader type. Specifically, average readers showed an increase of approximately 1,111 words 
+/- 57.63 se (t = 19.29, p &lt; 0.001) per year. Frequent readers showed an 
additional increase of 372 words +/- 77.44 se per year 
(1,484 words total, t = 4.81, p &lt; 0.001). 

---
class: middle, center

&lt;iframe src="https://gallery.shinyapps.io/multi_regression/" style="border:none;" height="600" width="1300"&gt;&lt;/iframe&gt;

---

### Practice

[dummy variable practice](./assets/dummies_walkthrough/dummies.zip)

---
exclude: true

[WG16; Har99; Har99; Har99]

---
layout: false
class: title-slide-final, left

# References

Hardy, M. "Assessing Group Differences in Effects". In:
_Regression with Dummy Variables_. Ed. by M. Hardy , pp. 29-63.

Hardy, M. "Creating Dummy Variables". In: _Regression with Dummy
Variables_. Ed. by M. Hardy , pp. 7-17.

Hardy, M. "Using Dummy Variables as Regressors". In: _Regression
with Dummy Variables_. Ed. by M. Hardy , pp. 18-28.

Wickham, H. and G. Grolemund (2016). _R for Data Science: Import,
Tidy, Transform, Visualize, and Model Data_. O'Reilly Media.

Figueredo, A. J. (2013). Continuous and Categorical Predictors. *Statistical Methods in Psychological Research*.

Figueredo, A. J. (2013). The General Linear Model: ANOVA. *Statistical Methods in Psychological Research*.
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="http://www.jvcasillas.com/ru_xaringan/js/ru_xaringan.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "default",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
