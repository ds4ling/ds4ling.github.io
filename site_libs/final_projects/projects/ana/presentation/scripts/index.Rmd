---
title: "L2 productions of English plosives"
subtitle: "Effects of training?"
author: "Ana Rinzler  |  Rutgers University | Center for Cognitive Science"
date: "4/24/2016"
output:
  xaringan::moon_reader:
    css: ['rladies', 'rutgers-fonts', 'rutgers']
    lib_dir: libs
    nature:
      self_contained: false
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r eval=FALSE, tidy=FALSE,message=FALSE, include=FALSE}
devtools::install_github("yihui/xaringan")
names(xaringan:::list_css())
```

```{r, load libraries, message=FALSE, include=FALSE}
library(tidyverse)
library(xaringan)
library(ggfortify)
library(broom)
library(kableExtra)
library(lme4)
library(Matrix)
library(MuMIn)
library(likelihood)
library(lmerTest)
library(doBy)
```
```{r, upload raw data, message=FALSE, include=FALSE}
lang_df <- read.csv("/Users/anarinzler/Desktop/cantonese_final/raw data/total_raw.csv")
```

```{r, tidy data, message=FALSE, include=FALSE}
tidy_df <- lang_df %>%
  select(., Participant, TrainingGroup, Word, VoicedUnvoiced, Position, Dur, PrePosttraining, OnsetMedialCoda, POA, tierNumber) %>%
  filter(., Position == "vowel", PrePosttraining == "post", OnsetMedialCoda == "coda", tierNumber == "1", Word != "got")
save(tidy_df,file="tidy_df.Rda")
```

```{r, message=FALSE, include=FALSE, echo=TRUE}
sum_df <- structure(list(Participant = c(1404L, 1404L, 1404L, 1404L, 1404L, 
1404L, 1404L, 1408L, 1408L, 1408L, 1408L, 1408L, 1408L, 1408L, 
1408L, 1410L, 1410L, 1410L, 1410L, 1410L, 1410L, 1410L, 1410L, 
1411L, 1411L, 1411L, 1411L, 1411L, 1411L, 1411L, 1411L, 1412L, 
1412L, 1412L, 1412L, 1412L, 1412L, 1412L, 1412L, 1413L, 1413L, 
1413L, 1413L, 1413L, 1413L, 1413L, 1413L, 1416L, 1416L, 1416L, 
1416L, 1416L, 1416L, 1416L, 1416L, 1422L, 1422L, 1422L, 1422L, 
1422L, 1422L, 1422L, 1422L, 1424L, 1424L, 1424L, 1424L, 1424L, 
1424L, 1424L, 1424L, 1430L, 1430L, 1430L, 1430L, 1430L, 1430L, 
1430L, 1430L, 1432L, 1432L, 1432L, 1432L, 1432L, 1432L, 1432L, 
1432L, 1433L, 1433L, 1433L, 1433L, 1433L, 1433L, 1433L, 1433L, 
1441L, 1441L, 1441L, 1441L, 1441L, 1441L, 1441L, 1441L, 1444L, 
1444L, 1444L, 1444L, 1444L, 1444L, 1444L, 1444L, 1445L, 1445L, 
1445L, 1445L, 1445L, 1445L, 1445L, 1445L, 1446L, 1446L, 1446L, 
1446L, 1446L, 1446L, 1446L, 1446L, 1448L, 1448L, 1448L, 1448L, 
1448L, 1448L, 1448L, 1448L, 1452L, 1452L, 1452L, 1452L, 1452L, 
1452L, 1452L, 1452L, 1401L, 1401L, 1401L, 1401L, 1401L, 1401L, 
1401L, 1401L, 1401L, 1405L, 1405L, 1405L, 1405L, 1405L, 1405L, 
1405L, 1405L, 1405L, 1417L, 1417L, 1417L, 1417L, 1417L, 1417L, 
1417L, 1417L, 1417L, 1418L, 1418L, 1418L, 1418L, 1418L, 1418L, 
1418L, 1418L, 1418L, 1419L, 1419L, 1419L, 1419L, 1419L, 1419L, 
1419L, 1419L, 1419L, 1420L, 1420L, 1420L, 1420L, 1420L, 1420L, 
1420L, 1420L, 1420L, 1423L, 1423L, 1423L, 1423L, 1423L, 1423L, 
1423L, 1423L, 1423L, 1427L, 1427L, 1427L, 1427L, 1427L, 1427L, 
1427L, 1427L, 1427L, 1431L, 1431L, 1431L, 1431L, 1431L, 1431L, 
1431L, 1431L, 1431L, 1435L, 1435L, 1435L, 1435L, 1435L, 1435L, 
1435L, 1435L, 1435L, 1436L, 1436L, 1436L, 1436L, 1436L, 1436L, 
1436L, 1436L, 1436L, 1437L, 1437L, 1437L, 1437L, 1437L, 1437L, 
1437L, 1437L, 1437L, 1439L, 1439L, 1439L, 1439L, 1439L, 1439L, 
1439L, 1439L, 1439L, 1440L, 1440L, 1440L, 1440L, 1440L, 1440L, 
1440L, 1440L, 1440L, 1443L, 1443L, 1443L, 1443L, 1443L, 1443L, 
1443L, 1443L, 1443L, 1447L, 1447L, 1447L, 1447L, 1447L, 1447L, 
1447L, 1447L, 1447L, 1449L, 1449L, 1449L, 1449L, 1449L, 1449L, 
1449L, 1449L, 1449L, 1454L, 1454L, 1454L, 1454L, 1454L, 1454L, 
1454L, 1454L, 1454L, 1401L, 1401L, 1401L, 1401L, 1401L, 1401L, 
1401L, 1405L, 1405L, 1405L, 1405L, 1405L, 1405L, 1405L, 1417L, 
1417L, 1417L, 1417L, 1417L, 1418L, 1418L, 1418L, 1418L, 1418L, 
1418L, 1418L, 1419L, 1419L, 1419L, 1419L, 1419L, 1419L, 1419L, 
1420L, 1420L, 1420L, 1420L, 1420L, 1420L, 1420L, 1423L, 1423L, 
1423L, 1423L, 1423L, 1423L, 1423L, 1427L, 1427L, 1427L, 1427L, 
1427L, 1427L, 1431L, 1431L, 1431L, 1431L, 1431L, 1431L, 1431L, 
1435L, 1435L, 1435L, 1435L, 1435L, 1435L, 1435L, 1436L, 1436L, 
1436L, 1436L, 1436L, 1436L, 1436L, 1437L, 1437L, 1437L, 1437L, 
1437L, 1437L, 1437L, 1439L, 1439L, 1439L, 1439L, 1439L, 1439L, 
1439L, 1440L, 1440L, 1440L, 1440L, 1440L, 1440L, 1440L, 1443L, 
1443L, 1443L, 1443L, 1443L, 1443L, 1447L, 1447L, 1447L, 1447L, 
1447L, 1447L, 1447L, 1449L, 1449L, 1449L, 1449L, 1449L, 1449L, 
1454L, 1454L, 1454L, 1454L, 1454L, 1454L, 1454L, 1404L, 1404L, 
1404L, 1404L, 1404L, 1404L, 1404L, 1404L, 1404L, 1408L, 1408L, 
1408L, 1408L, 1408L, 1408L, 1408L, 1408L, 1408L, 1410L, 1410L, 
1410L, 1410L, 1410L, 1410L, 1410L, 1410L, 1410L, 1411L, 1411L, 
1411L, 1411L, 1411L, 1411L, 1411L, 1411L, 1411L, 1412L, 1412L, 
1412L, 1412L, 1412L, 1412L, 1412L, 1412L, 1412L, 1413L, 1413L, 
1413L, 1413L, 1413L, 1413L, 1413L, 1413L, 1413L, 1416L, 1416L, 
1416L, 1416L, 1416L, 1416L, 1416L, 1416L, 1416L, 1422L, 1422L, 
1422L, 1422L, 1422L, 1422L, 1422L, 1422L, 1422L, 1424L, 1424L, 
1424L, 1424L, 1424L, 1424L, 1424L, 1424L, 1424L, 1430L, 1430L, 
1430L, 1430L, 1430L, 1430L, 1430L, 1430L, 1430L, 1432L, 1432L, 
1432L, 1432L, 1432L, 1432L, 1432L, 1432L, 1432L, 1433L, 1433L, 
1433L, 1433L, 1433L, 1433L, 1433L, 1433L, 1433L, 1441L, 1441L, 
1441L, 1441L, 1441L, 1441L, 1441L, 1441L, 1441L, 1444L, 1444L, 
1444L, 1444L, 1444L, 1444L, 1444L, 1444L, 1444L, 1445L, 1445L, 
1445L, 1445L, 1445L, 1445L, 1445L, 1445L, 1445L, 1446L, 1446L, 
1446L, 1446L, 1446L, 1446L, 1446L, 1446L, 1446L, 1448L, 1448L, 
1448L, 1448L, 1448L, 1448L, 1448L, 1448L, 1448L, 1452L, 1452L, 
1452L, 1452L, 1452L, 1452L, 1452L, 1452L, 1452L, 1401L, 1404L, 
1405L, 1408L, 1410L, 1411L, 1412L, 1413L, 1416L, 1417L, 1418L, 
1419L, 1420L, 1422L, 1423L, 1424L, 1427L, 1430L, 1431L, 1432L, 
1433L, 1435L, 1436L, 1437L, 1439L, 1440L, 1441L, 1443L, 1444L, 
1445L, 1446L, 1447L, 1448L, 1449L, 1452L, 1454L, 1404L, 1417L, 
1401L, 1405L, 1417L, 1417L, 1418L, 1419L, 1420L, 1423L, 1427L, 
1427L, 1431L, 1435L, 1436L, 1437L, 1439L, 1440L, 1443L, 1443L, 
1447L, 1449L, 1449L, 1454L), TrainingGroup = structure(c(5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 3L, 5L, 3L, 5L, 5L, 
5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 5L, 3L, 5L, 3L, 5L, 3L, 5L, 5L, 
3L, 3L, 3L, 3L, 3L, 5L, 3L, 5L, 5L, 5L, 3L, 5L, 3L, 5L, 3L, 5L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = c("", " ", "No", "yes", 
"Yes"), class = "factor"), Word = structure(c(3L, 7L, 13L, 14L, 
17L, 19L, 27L, 3L, 7L, 11L, 13L, 14L, 17L, 19L, 27L, 3L, 7L, 
11L, 13L, 14L, 17L, 19L, 27L, 3L, 7L, 11L, 13L, 14L, 17L, 19L, 
27L, 3L, 7L, 11L, 13L, 14L, 17L, 19L, 27L, 3L, 7L, 11L, 13L, 
14L, 17L, 19L, 27L, 3L, 7L, 11L, 13L, 14L, 17L, 19L, 27L, 3L, 
7L, 11L, 13L, 14L, 17L, 19L, 27L, 3L, 7L, 11L, 13L, 14L, 17L, 
19L, 27L, 3L, 7L, 11L, 13L, 14L, 17L, 19L, 27L, 3L, 7L, 11L, 
13L, 14L, 17L, 19L, 27L, 3L, 7L, 11L, 13L, 14L, 17L, 19L, 27L, 
3L, 7L, 11L, 13L, 14L, 17L, 19L, 27L, 3L, 7L, 11L, 13L, 14L, 
17L, 19L, 27L, 3L, 7L, 11L, 13L, 14L, 17L, 19L, 27L, 3L, 7L, 
11L, 13L, 14L, 17L, 19L, 27L, 3L, 7L, 11L, 13L, 14L, 17L, 19L, 
27L, 3L, 7L, 11L, 13L, 14L, 17L, 19L, 27L, 4L, 5L, 10L, 12L, 
15L, 16L, 18L, 25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 25L, 
26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 25L, 26L, 4L, 5L, 10L, 
12L, 15L, 16L, 18L, 25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 
25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 25L, 26L, 4L, 5L, 
10L, 12L, 15L, 16L, 18L, 25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 
18L, 25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 25L, 26L, 4L, 
5L, 10L, 12L, 15L, 16L, 18L, 25L, 26L, 4L, 5L, 10L, 12L, 15L, 
16L, 18L, 25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 25L, 26L, 
4L, 5L, 10L, 12L, 15L, 16L, 18L, 25L, 26L, 4L, 5L, 10L, 12L, 
15L, 16L, 18L, 25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 25L, 
26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 25L, 26L, 4L, 5L, 10L, 
12L, 15L, 16L, 18L, 25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 
25L, 26L, 3L, 7L, 11L, 13L, 17L, 19L, 27L, 3L, 7L, 11L, 13L, 
17L, 19L, 27L, 3L, 7L, 11L, 13L, 27L, 3L, 7L, 11L, 13L, 17L, 
19L, 27L, 3L, 7L, 11L, 13L, 17L, 19L, 27L, 3L, 7L, 11L, 13L, 
17L, 19L, 27L, 3L, 7L, 11L, 13L, 17L, 19L, 27L, 3L, 7L, 11L, 
13L, 17L, 19L, 3L, 7L, 11L, 13L, 17L, 19L, 27L, 3L, 7L, 11L, 
13L, 17L, 19L, 27L, 3L, 7L, 11L, 13L, 17L, 19L, 27L, 3L, 7L, 
11L, 13L, 17L, 19L, 27L, 3L, 7L, 11L, 13L, 17L, 19L, 27L, 3L, 
7L, 11L, 13L, 17L, 19L, 27L, 3L, 7L, 11L, 13L, 17L, 19L, 3L, 
7L, 11L, 13L, 17L, 19L, 27L, 3L, 7L, 11L, 13L, 17L, 19L, 3L, 
7L, 11L, 13L, 17L, 19L, 27L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 
25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 25L, 26L, 4L, 5L, 
10L, 12L, 15L, 16L, 18L, 25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 
18L, 25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 25L, 26L, 4L, 
5L, 10L, 12L, 15L, 16L, 18L, 25L, 26L, 4L, 5L, 10L, 12L, 15L, 
16L, 18L, 25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 25L, 26L, 
4L, 5L, 10L, 12L, 15L, 16L, 18L, 25L, 26L, 4L, 5L, 10L, 12L, 
15L, 16L, 18L, 25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 25L, 
26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 25L, 26L, 4L, 5L, 10L, 
12L, 15L, 16L, 18L, 25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 
25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 25L, 26L, 4L, 5L, 
10L, 12L, 15L, 16L, 18L, 25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 
18L, 25L, 26L, 4L, 5L, 10L, 12L, 15L, 16L, 18L, 25L, 26L, 24L, 
24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 
24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 
24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 11L, 17L, 14L, 14L, 
14L, 19L, 14L, 14L, 14L, 14L, 14L, 27L, 14L, 14L, 14L, 14L, 14L, 
14L, 27L, 14L, 14L, 27L, 14L, 14L), .Label = c("", " ", "back", 
"bad", "bag", "bagger", "bat", "bigger", "bobbing", "cab", "cap", 
"cub", "cup", "dock", "dog", "fad", "fat", "feed", "feet", "got", 
"lagging", "nabbing", "nibble", "pick", "pig", "tab", "tap"), class = "factor"), 
    VoicedUnvoiced = structure(c(3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = c("", 
    " ", "unvoiced", "voiced"), class = "factor"), Position = structure(c(6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
    6L, 6L), .Label = c("", " ", "coda", "medial", "onset", "vowel"
    ), class = "factor"), Dur = c(236L, 259L, 158L, 186L, 194L, 
    235L, 185L, 271L, 208L, 243L, 136L, 205L, 271L, 187L, 263L, 
    213L, 178L, 204L, 125L, 189L, 219L, 164L, 181L, 139L, 140L, 
    133L, 104L, 151L, 150L, 112L, 111L, 215L, 168L, 133L, 155L, 
    187L, 174L, 217L, 143L, 181L, 193L, 163L, 151L, 180L, 178L, 
    162L, 196L, 212L, 236L, 187L, 102L, 194L, 200L, 181L, 196L, 
    196L, 220L, 173L, 169L, 231L, 211L, 183L, 161L, 239L, 168L, 
    133L, 156L, 200L, 224L, 247L, 132L, 200L, 184L, 90L, 109L, 
    168L, 188L, 112L, 139L, 200L, 198L, 101L, 87L, 278L, 194L, 
    239L, 118L, 200L, 186L, 134L, 139L, 178L, 183L, 176L, 198L, 
    209L, 226L, 179L, 79L, 183L, 204L, 176L, 204L, 215L, 160L, 
    130L, 103L, 195L, 221L, 215L, 119L, 188L, 133L, 142L, 85L, 
    159L, 153L, 190L, 121L, 164L, 199L, 165L, 152L, 207L, 194L, 
    164L, 251L, 219L, 162L, 152L, 115L, 217L, 154L, 154L, 164L, 
    182L, 173L, 131L, 146L, 155L, 155L, 108L, 143L, 246L, 247L, 
    185L, 133L, 198L, 213L, 237L, 182L, 178L, 210L, 178L, 132L, 
    176L, 181L, 182L, 170L, 134L, 164L, 256L, 261L, 230L, 189L, 
    263L, 260L, 303L, 193L, 235L, 199L, 205L, 216L, 128L, 221L, 
    203L, 169L, 163L, 183L, 218L, 227L, 134L, 135L, 174L, 208L, 
    139L, 144L, 91L, 201L, 188L, 214L, 134L, 171L, 221L, 257L, 
    116L, 162L, 146L, 136L, 142L, 116L, 155L, 148L, 167L, 127L, 
    126L, 199L, 170L, 172L, 113L, 179L, 184L, 209L, 132L, 162L, 
    250L, 252L, 200L, 116L, 169L, 212L, 193L, 176L, 191L, 184L, 
    235L, 167L, 159L, 161L, 200L, 235L, 142L, 140L, 315L, 314L, 
    116L, 133L, 173L, 230L, 380L, 173L, 164L, 222L, 246L, 207L, 
    144L, 248L, 253L, 248L, 217L, 187L, 353L, 251L, 213L, 190L, 
    227L, 240L, 299L, 208L, 226L, 226L, 221L, 156L, 122L, 217L, 
    219L, 260L, 121L, 164L, 163L, 185L, 98L, 94L, 144L, 178L, 
    181L, 111L, 168L, 170L, 177L, 145L, 123L, 138L, 160L, 213L, 
    149L, 136L, 97L, 139L, 67L, 72L, 102L, 113L, 121L, 97L, 68L, 
    231L, 179L, 158L, 95L, 178L, 229L, 244L, 139L, 172L, 253L, 
    234L, 178L, 184L, 211L, 240L, 172L, 172L, 193L, 133L, 152L, 
    145L, 145L, 135L, 251L, 249L, 243L, 166L, 234L, 193L, 202L, 
    148L, 150L, 184L, 153L, 189L, 134L, 187L, 163L, 153L, 164L, 
    150L, 166L, 191L, 159L, 106L, 106L, 203L, 292L, 178L, 167L, 
    157L, 125L, 82L, 161L, 180L, 148L, 208L, 201L, 150L, 118L, 
    207L, 174L, 149L, 222L, 208L, 129L, 149L, 173L, 200L, 198L, 
    222L, 167L, 119L, 173L, 229L, 143L, 202L, 172L, 250L, 107L, 
    254L, 341L, 154L, 200L, 187L, 166L, 129L, 167L, 168L, 160L, 
    243L, 204L, 160L, 168L, 228L, 262L, 179L, 225L, 196L, 179L, 
    108L, 239L, 259L, 131L, 195L, 167L, 135L, 109L, 198L, 165L, 
    119L, 150L, 137L, 115L, 139L, 150L, 135L, 156L, 162L, 86L, 
    70L, 145L, 137L, 218L, 238L, 226L, 157L, 213L, 219L, 207L, 
    226L, 247L, 135L, 105L, 217L, 202L, 175L, 152L, 198L, 309L, 
    321L, 261L, 173L, 312L, 264L, 222L, 225L, 304L, 255L, 297L, 
    183L, 94L, 233L, 218L, 245L, 210L, 204L, 162L, 175L, 130L, 
    83L, 91L, 173L, 107L, 87L, 125L, 174L, 179L, 112L, 106L, 
    171L, 192L, 148L, 129L, 167L, 180L, 208L, 148L, 115L, 184L, 
    187L, 176L, 148L, 218L, 289L, 261L, 257L, 166L, 274L, 300L, 
    286L, 177L, 320L, 277L, 285L, 200L, 150L, 257L, 280L, 269L, 
    189L, 190L, 227L, 200L, 160L, 130L, 190L, 301L, 260L, 99L, 
    179L, 240L, 200L, 123L, 70L, 177L, 187L, 167L, 109L, 189L, 
    215L, 291L, 256L, 166L, 236L, 256L, 292L, 200L, 220L, 220L, 
    250L, 220L, 115L, 186L, 200L, 270L, 176L, 210L, 233L, 250L, 
    168L, 130L, 250L, 224L, 209L, 187L, 229L, 220L, 237L, 130L, 
    117L, 210L, 192L, 219L, 93L, 145L, 186L, 170L, 104L, 70L, 
    227L, 184L, 184L, 93L, 110L, 231L, 232L, 201L, 164L, 209L, 
    200L, 230L, 124L, 172L, 255L, 313L, 206L, 190L, 260L, 250L, 
    254L, 141L, 219L, 235L, 204L, 133L, 110L, 190L, 200L, 238L, 
    93L, 154L, 105L, 121L, 75L, 162L, 139L, 78L, 70L, 76L, 94L, 
    106L, 124L, 85L, 71L, 112L, 87L, 78L, 96L, 40L, 118L, 98L, 
    82L, 91L, 92L, 105L, 117L, 133L, 84L, 105L, 76L, 58L, 100L, 
    99L, 83L, 38L, 43L, 67L, 146L, 212L, 211L, 151L, 205L, 204L, 
    135L, 156L, 177L, 154L, 162L, 98L, 186L, 191L, 215L, 166L, 
    185L, 288L, 121L, 134L, 116L, 79L, 141L, 211L), PrePosttraining = structure(c(3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L), .Label = c("", " ", "post", "pre"), class = "factor"), 
    OnsetMedialCoda = structure(c(3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = c("", 
    " ", "coda", "medial"), class = "factor"), POA = structure(c(5L, 
    3L, 4L, 5L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 5L, 3L, 3L, 4L, 5L, 
    3L, 4L, 4L, 5L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 5L, 3L, 3L, 4L, 
    5L, 3L, 4L, 4L, 5L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 5L, 3L, 3L, 
    4L, 5L, 3L, 4L, 4L, 5L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 5L, 3L, 
    3L, 4L, 5L, 3L, 4L, 4L, 5L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 5L, 
    3L, 3L, 4L, 5L, 3L, 4L, 4L, 5L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 
    5L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 5L, 3L, 3L, 4L, 5L, 3L, 4L, 
    4L, 5L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 5L, 3L, 3L, 4L, 5L, 3L, 
    4L, 4L, 5L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 5L, 3L, 3L, 4L, 5L, 
    3L, 4L, 4L, 5L, 3L, 3L, 4L, 3L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 
    4L, 3L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 3L, 5L, 4L, 4L, 5L, 
    3L, 3L, 5L, 4L, 3L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 3L, 5L, 
    4L, 4L, 5L, 3L, 3L, 5L, 4L, 3L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 
    4L, 3L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 3L, 5L, 4L, 4L, 5L, 
    3L, 3L, 5L, 4L, 3L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 3L, 5L, 
    4L, 4L, 5L, 3L, 3L, 5L, 4L, 3L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 
    4L, 3L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 3L, 5L, 4L, 4L, 5L, 
    3L, 3L, 5L, 4L, 3L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 3L, 5L, 
    4L, 4L, 5L, 3L, 3L, 5L, 4L, 3L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 
    4L, 3L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 3L, 5L, 4L, 4L, 5L, 
    3L, 3L, 5L, 4L, 5L, 3L, 4L, 4L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 
    3L, 3L, 4L, 5L, 3L, 4L, 4L, 3L, 5L, 3L, 4L, 4L, 3L, 3L, 4L, 
    5L, 3L, 4L, 4L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 3L, 3L, 4L, 5L, 
    3L, 4L, 4L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 3L, 3L, 5L, 3L, 4L, 
    4L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 
    3L, 3L, 4L, 5L, 3L, 4L, 4L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 3L, 
    3L, 4L, 5L, 3L, 4L, 4L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 3L, 3L, 
    5L, 3L, 4L, 4L, 3L, 3L, 4L, 5L, 3L, 4L, 4L, 3L, 3L, 5L, 3L, 
    4L, 4L, 3L, 3L, 4L, 4L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 4L, 
    5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 4L, 5L, 4L, 4L, 5L, 3L, 3L, 
    5L, 4L, 4L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 4L, 5L, 4L, 4L, 
    5L, 3L, 3L, 5L, 4L, 4L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 4L, 
    5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 4L, 5L, 4L, 4L, 5L, 3L, 3L, 
    5L, 4L, 4L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 4L, 5L, 4L, 4L, 
    5L, 3L, 3L, 5L, 4L, 4L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 4L, 
    5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 4L, 5L, 4L, 4L, 5L, 3L, 3L, 
    5L, 4L, 4L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 4L, 5L, 4L, 4L, 
    5L, 3L, 3L, 5L, 4L, 4L, 5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 4L, 
    5L, 4L, 4L, 5L, 3L, 3L, 5L, 4L, 4L, 5L, 4L, 4L, 5L, 3L, 3L, 
    5L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 5L, 5L, 5L, 3L, 5L, 
    5L, 5L, 5L, 5L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 3L, 5L, 5L, 4L, 
    5L, 5L), .Label = c("", " ", "alveolar", "bilabial", "velar"
    ), class = "factor"), tierNumber = c(1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), trainingSum = c(1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 
    1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, 1, 
    1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, 1, -1, 1, -1, 
    1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1), voicingSum = c(-1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1)), class = "data.frame", .Names = c("Participant", 
"TrainingGroup", "Word", "VoicedUnvoiced", "Position", "Dur", 
"PrePosttraining", "OnsetMedialCoda", "POA", "tierNumber", "trainingSum", 
"voicingSum"), row.names = c(NA, -648L))


```
```{r sum coding, message=FALSE, include=FALSE, echo=TRUE}
# Get rid of unused levels
sum_df <- droplevels(sum_df)
#Set participant as factor, recode training and voicing
# normalize duration by participant
sum_df <- mutate(sum_df, Participant = as.factor(Participant), 
                         trainingSum = if_else(TrainingGroup == "Yes", 1, -1), 
                         voicingSum = if_else(VoicedUnvoiced == "voiced", 1, -1)) %>% 
  group_by(., Participant) %>% 
  mutate(., durNorm = (Dur - mean(Dur)) / sd(Dur)) %>% 
  ungroup(.)

```
class: inverse, center, middle

# Background & Research Question

---
class: left, middle 

# Background: Timing vs. Amount of Input 

--

.pull-left[
### Critical Period Hypothesis (Lenneberg, 1967):
- critical period for acquisition is birth to puberty
- this applies to both L1 and L2
- learning a language after puberty is more difficult 
- difficulty is due to reduction in neural plasticity with age?


]

--

.pull-right[
### 'Input Hypothesis' (Au, ms):
- adults receive less exposure 
- optimal acquisition of English as L2 differs by L1
- Spanish L1 speakers can acquire English 
  with native-like quality until 15 yrs. (Birdsong & Molis, 2001)
]  
--

**Can the quality and quantity of input influence a Cantonese speaker's ability to perceive and produce notoriously difficult English contrasts?**

---
# What are the English contrasts of interest?  
**English Plosives:** /p, t, k/ and /b, d, g/

- Bilabial: /p/ is **voiceless**; /b/ is **voiced**
- Alveolar: /t/ is **voiceless**; /d/ is **voiced**
- Velar: /k/ is **voiceless**; /g/ is **voiced** 

--

### What makes these contrasts difficult? 

--
.pull-left[
**Cantonese** /p, t, k/ and /b, d, g/
- Cantonese does not have articulatory feature of **voicing** 
- Cantonese uses **aspiration** to make these distinction in onset position
- Cantonese **only** has unreleased /p, t, k/ in coda position
]
--
.pull-right[
**English:** /p, t, k/ and /b, d, g/
- In English /p, t, k/ are **aspirated** (i.e. in onset position: [**p**æt] vs. [**b**æt])
- In English /p, t, k/ are **unaspirated** and released (i.e. in coda position: [tæ**p**] vs. [tæ**b**])
] 

Chan and Li (2000)

Note: _I will focus on words with stops in coda position_

---
class: left, middle 

# Main Question of interest

--

## Does training result in trained Cantonese speakers producing longer vowel  durations in words with voiced stops in coda position? 

--

# Prediction

--

Cantonese speakers that received training will produce longer vowel lengths in words with voiced stops in coda position than Cantonese speakers that _did not_ receive training.  

**Note:** _The training and collection of Cantonese speakers data was done by Terry Au (ms). She generously permitted us to use her data for acoustic analyses_

---

# Why hypothesize this?

- Vowel duration is an acoustic cue to stops in coda position in English 
- Vowel duration is **longer** before voiced consonants than for voiceless
 
(Charles-Luce, 1985; House and Fairbanks, 1953; Peterson and Lehiste, 1960;
House, 1961; Umeda, 1975; Klatt, 1976)  


--
Perhaps Cantonese speakers utilize this cue to distinguish between voiced and voiceless consonants.  

--

**Are Cantonese speakers producing these words like native English speakers; or are they doing something different?**
---
class: inverse, center, middle

# Methods

---
# Methods

Participants (18- 22 yrs.)
- 18 trained adult Cantonese speakers 
- 18 waitlist-control adult Cantonese speakers 

--

Data
- Productions of phonological minimal word pairs with **voiced** and **voiceless** stops in coda position after training 

--

.pull-left[
**Voiced**
- bad
- bag 
- cab
- cub
- dog
- fad
- feed 
- pig
- tab 
]

--

.pull-right[
**Voiceless**
- bat
- back
- cap
- cup
- dock
- fat
- feet
- pick
- tab 
]



---

# Acoustic Slicing
- Used PRAAT
- Marked vowel duration boundaries for all productions 
- Utilized the wav method for the beginning of the vowel and F2 method for the end of the vowel 


--
Categorical Predictors: **Fixed Effects**
- Training: 2 levels (untrained/trained)
- Voicing: 2 levels (voiceless/voiced)

Categorical Predictors: **Random Effect**
- Participant


--

Criterion
- Vowel duration (ms) of post productions with voiced and voiceless plosives in coda position 

---
# Data 

### This is the tidy data: 
```{r view tidy data, message=FALSE, echo=FALSE}
tidy_df <- droplevels(tidy_df)
head(tidy_df,5)
```
---
# Data 

### Summary of stats
```{r, message=FALSE, echo=FALSE}
summaryBy(Dur ~ TrainingGroup + VoicedUnvoiced, data = tidy_df, 
 	FUN = function(x) { c(Mean = mean(x), StandDev = sd(x)) } )
```

---
# Viewing the data
```{r, message=FALSE,fig.retina=2, fig.align='center', fig.width=8,fig.height= 6, echo=FALSE}
tidy_df %>% 
  ggplot(., aes(x = Participant, y = Dur, fill = VoicedUnvoiced)) +
  geom_boxplot() +
  facet_grid(. ~ TrainingGroup) +
  labs(x = "Training", y = "Vowel Duration(ms)", title = "Effects of Training on Vowel Duration", fill = "Voiced or Unvoiced")
```

Overall, it appears that trained and untrained participants had about equal vowel lengths - with longer vowel lengths for in the voiced trained condition

---
# Analysis: General Linear Mixed Effect Model 

### Data Manipulation
- Categorical variables were sum coded

--

- Created 2 new columns for the training and voicing variables: trained (1), untrained (-1), voiced (1), voiceless (-1)

--

- The factor **participants** was set as a random factor: because there were multiple responses from the same subject (i.e. they produced more than one word), thus, these productions are _not_ independent from each other  

--

- Duration was normalized by participant

```{r, message=FALSE, include=FALSE, echo=FALSE, warning=FALSE}
mod_full <- lmer(Dur ~ voicingSum * trainingSum + 
                (1 | Participant) + 
                (1 | Participant:trainingSum),
                control = lmerControl(optimizer = 'bobyqa'), 
                data = sum_df, REML=FALSE)
summary(mod_full)
r.squaredGLMM(mod_full)
#R2m (fixed) = .05244 R@c = .30500

#additve model 
mod_add <- lmer(Dur ~ voicingSum + trainingSum + 
                (1 | Participant), 
                control = lmerControl(optimizer = 'bobyqa'), 
                data = sum_df, REML=FALSE)
summary(mod_add)
r.squaredGLMM(mod_add)
#R2m = .04829 R2c = .3009

# no voicing
mod_train <- lmer(Dur ~ trainingSum + 
                (1 | Participant), 
                control = lmerControl(optimizer = 'bobyqa'), 
                data = sum_df, REML=FALSE)
summary(mod_train)
r.squaredGLMM(mod_train)
#R2m = .00332 R2c= .2538

# no training
mod_voice <- lmer(Dur ~ voicingSum + 
                (1 | Participant), 
                control = lmerControl(optimizer = 'bobyqa'), 
                data = sum_df, REML=FALSE)
summary(mod_voice)
r.squaredGLMM(mod_voice)
# R2m = .04536 R2c= .2951

# null mod
mod_null <- lmer(Dur ~ 
                (1 | Participant), 
                control = lmerControl(optimizer = 'bobyqa'), 
                data = sum_df, REML=FALSE)
summary(mod_null)
r.squaredGLMM(mod_null)
# R2m = .00000 R2c= .2476
```
---
# Testing Model Assumptions 

--
### Normality of Residuals

```{r, message=FALSE, echo=FALSE, fig.height= 5.5, fig.align= 'center', fig.retina= 2}
qqnorm(residuals(mod_full))
qqline(residuals(mod_full))
```

---
# Testing Model Assumptions

--
### Homoskedasticity 
This now looks fine with participant as a random factor 
```{r,message=FALSE, echo=FALSE, fig.height= 5, fig.align= 'center', fig.retina= 2}
plot(fitted(mod_full), residuals(mod_full))
```

---
# Analysis: General Linear Mixed Effects Model
```{r, message=FALSE, warning=FALSE, fig.height = 4}
head(sum_df)
```

---
# Nested Model Comparisons

### Tested the following models:
- **Inclusive:** (duration ~ voicingSum * trainingSum )
- ** Additive:**  (duration ~ voicingSum + trainingSum)
- ** No Voicing:**  (duration ~ trainingSum)
- ** No Training:** (duration ~ voicingSum)
- **Null:**  (duration ~ 1)

--

Note that a **(1|Participant)** notation was added to each model. The 1 represents the assumption that there are different intercepts for each subject due to have multiple responses for each per participant.

```{r, message=FALSE, include=FALSE, echo=FALSE, warning=FALSE}
mod_full <- lmer(Dur ~ voicingSum * trainingSum + 
                (1 | Participant) + 
                (1 | Participant:trainingSum),
                control = lmerControl(optimizer = 'bobyqa'), 
                data = sum_df, REML=FALSE)
summary(mod_full)
r.squaredGLMM(mod_full)
#R2m (fixed) = .05244 R@c = .30500

#additve model 
mod_add <- lmer(Dur ~ voicingSum + trainingSum + 
                (1 | Participant), 
                control = lmerControl(optimizer = 'bobyqa'), 
                data = sum_df, REML=FALSE)
summary(mod_add)
r.squaredGLMM(mod_add)
#R2m = .04829 R2c = .3009

# no voicing
mod_train <- lmer(Dur ~ trainingSum + 
                (1 | Participant), 
                control = lmerControl(optimizer = 'bobyqa'), 
                data = sum_df, REML=FALSE)
summary(mod_train)
r.squaredGLMM(mod_train)
#R2m = .00332 R2c= .2538

# no training
mod_voice <- lmer(Dur ~ voicingSum + 
                (1 | Participant), 
                control = lmerControl(optimizer = 'bobyqa'), 
                data = sum_df, REML=FALSE)
summary(mod_voice)
r.squaredGLMM(mod_voice)
# R2m = .04536 R2c= .2951

# null mod
mod_null <- lmer(Dur ~ 
                (1 | Participant), 
                control = lmerControl(optimizer = 'bobyqa'), 
                data = sum_df, REML=FALSE)
summary(mod_null)
r.squaredGLMM(mod_null)
# R2m = .00000 R2c= .2476
```

---

# Best Model Summary

---
The inclusive model was the best model fit. It explained the most variance, with R squared marginal = .052 and conditional = .305
```{r, best model print, message=FALSE, warning=FALSE, echo=FALSE}
summary(mod_full)
```

---
# Best Model Summary

**Marginal R squared:** the proportion of variance explained _only_ including on the fixed factors (= 5.2%)  

**Conditional R squared:** the the proportion of variance explained including _both_ the fixed and random factors (= 30.5%)  

--
The inclusive model explained more variance than just voicing or training alone 

**Model with only voicing:** R squared marginal = .045, conditional = .295  

**Model with only training:** R squared marginal =  .003, conditional= .253

--
We can see that training explained only a small part of the variance, while, 
and that conditional variance (including the random factor participant), explained more than 
the fixed factors. 

```{r, message=FALSE, warning=FALSE, echo=FALSE, include=FALSE}
anova(mod_null, mod_full) # test int
anova(mod_null, mod_voice) # test voicing
anova(mod_null, mod_train) # test training

```
---
# Comparing Models 

- **Null** compared to **No Voicing** 
- **Null** compared to **No Training**
- **Null** compared to **Inclusive**

--

### Results

- **Main Effect of voicing:** χ2 (1)=44.664, p <.001
- **No Main Effect of traininng**: χ2 (1)=0.4302, p =0.5119
- **Significant Interaction between voicing and training:** χ2 (4)=40.342, p <.05

---
# Conclusions

- Words with **voiced** plosives had a mean vowel duration of approximately 189.63 (ms) +/- 1.8 (se); and words with **voiceless** plosives had a mean vowel duration of approximately 166.34 (ms) +/- 1.8 (se)  

- Training did not affect vowel duration  

- The significant interaction indicates an interdependence between voicing and training. 
- When the interaction between _voicing_ and _training_ was considered,
the vowel duration mean increased _voiced_ plosive duration to 193.19 (ms) +/- 4.805 (se) and _voiceless_ plosive duration  to 162.78 (ms) +/- 4.805 (se) (3.556 ms, ± 1.8 SE)



--
Overall, it seems that participants produced words with **voiced** plosives longer than words with **voiceless** plosives and that this effect was modulated by training. However, it can cannot be concluded that trained participants produced signficantly longer vowels than untrained participants. 


---
# Implications 

**Possible explanations for these patterns?**

.pull-left[
- Participant variability (recall SE of ± 4.9)  

- Excluding relevant variable  

- There is an affect of word?

]


--
.pull-right[
- Vowel length variation is language-specific to English, not a universal behavior  

- Is it a lengthening or a shortening rule?  

- Even within English speakers, it varies  
]

(Gandour, Weinberg & Rutokswki, 1980) 

---
# A glimpse at individual participants
```{r, message=FALSE, echo=FALSE, fig.retina=2, fig.align='center', fig.width=12,fig.height= 8}
sum_df %>% 
  ggplot(., aes(x = VoicedUnvoiced, y = durNorm, width= .10, fill = as.factor(Participant)))+
  geom_boxplot()+
  facet_grid(. ~ TrainingGroup) +
  labs(x = "Voiced or Unvoiced", y = "Vowel Duration(ms)", title = 
         "Duration by Participant", 
       fill = "Participant")
```

---
# A glimplse at individual word variability 

```{r, message=FALSE, echo=FALSE, include=TRUE, fig.retina=2, fig.align='center', fig.width=11,fig.height= 7}
sum_df %>% 
  ggplot(., aes(x = Word, y = Dur, fill = TrainingGroup)) +
  geom_boxplot() +
   labs(x = "Word", y = "Vowel Duration(ms)", 
       fill = "Training Group")
```

---
# Overall Conclusions:

- Training did not have an effect, **however...**  

- Vowel duration may not as robust of an acoustic cue as thought to be  

- Participants are most likely doing something different from native English speakers

- This may be espeically true for individuals who do not have a voicing contrast in their language  

- Excluded relevant variable (Type II error?) 


--

**Much variance left to be explained**
Recall: best model only explained ~ 5% of the variance :(

---
# How to manage this?

Perhaps vowel duration is not a cue that Cantonese speaker's utilize when making this distinction  

### What other predictors do I have?
- Place of articulation (POA)
- Phoneme 
- Performance on comprehension task 


### What other criterion do I plan to measure?
- VOT
- Mean aspiration intensity
- Aspiration duration 
- F2 at the end of the vowel 
- Closure duration 
- Voicing bar duration
- **Pre and post measures**


---
# Future Directions

### Better Understand what acoustic cues may be more relevant for Cantonese speakers learning English

- Explore the role of aspiration  

- Compare pre and post production measurements  

- To understand the Critical Period Hypothesis compare productions from younger participants to that of older participants  

- Use performance on comprehension task to understand individual variation in production   

---
# Thanks everyone for a great semester!
